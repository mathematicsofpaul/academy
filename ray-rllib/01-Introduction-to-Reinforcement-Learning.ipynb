{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Introduction to Reinforcement Learning\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "_Reinforcement Learning_ is the category of machine learning that focuses on training one or more _agents_ to achieve maximal _rewards_ while operating in an environment. This lesson discusses the core concepts of RL, while subsequent lessons explore RLlib in depth. We'll use two examples with exercises to give you a taste of RL. If you already understand RL concepts, you can either skim this lesson or skip to the [next lesson](02-Introduction-to-RLlib.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Reinforcement Learning?\n",
    "\n",
    "Let's explore the basic concepts of RL, specifically the _Markov Decision Process_ abstraction, and to show its use in Python.\n",
    "\n",
    "Consider the following image:\n",
    "\n",
    "![RL Concepts](../images/rllib/RL-concepts.png)\n",
    "\n",
    "In RL, one or more **agents** interact with an **environment** to maximize a **reward**. The agents make **observations** about the **state** of the environment and take **actions** that are believed will maximize the long-term reward. However, at any particular moment, the agents can only observe the immediate reward. So, the training process usually involves lots and lot of replay of the game, the robot simulator traversing a virtual space, etc., so the agents can learn from repeated trials what decisions/actions work best to maximize the long-term, cumulative reward.\n",
    "\n",
    "The trail and error search and delayed reward are the distinguishing characterists of RL vs. other ML methods ([Sutton 2018](06-RL-References.ipynb#Books)).\n",
    "\n",
    "The way to formalize trial and error is the **exploitation vs. exploration tradeoff**. When an agent finds what appears to be a \"rewarding\" sequence of actions, the agent may naturally want to continue to **exploit** these actions. However, even better actions may exist. An agent won't know whether alternatives are better or not unless some percentage of actions taken **explore** the alternatives. So, all RL algorithms include a strategy for exploitation and exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Applications\n",
    "\n",
    "RL has many potential applications. RL became \"famous\" due to these successes, including achieving expert game play, training robots, autonomous vehicles, and other simulated agents:\n",
    "\n",
    "![AlphaGo](../images/rllib/alpha-go.jpg)\n",
    "![Game](../images/rllib/breakout.png)\n",
    "\n",
    "![Stacking Legos with Sawyer](../images/rllib/stacking-legos-with-sawyer.gif)\n",
    "![Walking Man](../images/rllib/walking-man.gif)\n",
    "\n",
    "![Autonomous Vehicle](../images/rllib/daimler-autonomous-car.jpg)\n",
    "![\"Cassie\": Two-legged Robot](../images/rllib/cassie-crouched.png)\n",
    "\n",
    "Credits:\n",
    "* [AlphaGo](https://www.youtube.com/watch?v=l7ngy56GY6k)\n",
    "* [Breakout](https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756) ([paper](https://arxiv.org/abs/1312.5602))\n",
    "* [Stacking Legos with Sawyer](https://robohub.org/soft-actor-critic-deep-reinforcement-learning-with-real-world-robots/)\n",
    "* [Walking Man](https://openai.com/blog/openai-baselines-ppo/)\n",
    "* [Autonomous Vehicle](https://www.daimler.com/innovation/case/autonomous/intelligent-drive-2.html)\n",
    "* [\"Cassie\": Two-legged Robot](https://mime.oregonstate.edu/research/drl/robots/cassie/) (Uses Ray!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently other industry applications have emerged, include the following:\n",
    "\n",
    "* **Process optimization:** industrial processes (factories, pipelines) and other business processes, routing problems, cluster optimization.\n",
    "* **Ad serving and recommendations:** Some of the traditional methods, including _collaborative filtering_, are hard to scale for very large data sets. RL systems are being developed to do an effective job more efficiently than traditional methods.\n",
    "* **Finance:** Markets are time-oriented _environments_ where automated trading systems are the _agents_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## Markov Decision Processes\n",
    "\n",
    "At its core, Reinforcement learning builds on the concepts of [Markov Decision Process (MDP)](https://en.wikipedia.org/wiki/Markov_decision_process), where the current state, the possible actions that can be taken, and overall goal are the building blocks.\n",
    "\n",
    "An MDP models sequential interactions with an external environment. It consists of the following:\n",
    "\n",
    "- a **state space** where the current state of the system is sometimes called the **context**.\n",
    "- a set of **actions** that can be taken at a particular state $s$ (or sometimes the same set for all states).\n",
    "- a **transition function** describes the probability of being in a state $s'$ at time $t+1$ given that the MDP was in state $s$ at time $t$ and action $a$ was taken. The next state is selected stochastically based on these probabilities.\n",
    "- a **reward function**, which determines the reward received at time $t$ following action $a$, based on the decision of **policy** $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "The goal of MDP is to develop a **policy** $\\pi$ that specifies what action $a$ should be chosen for a given state $s$ so that the cumulative reward is maximized. When it is possible for the policy \"trainer\" to fully observe all the possible states, actions, and rewards, it can define a deterministic policy, fixing a single action choice for each state. In this scenario, the transition probabilities reduce to the probability of transitioning to state $s'$ given the current state is $s$, independent of actions, because the state now leads to a deterministic action choice. Various algorithms can be used to compute this policy. \n",
    "\n",
    "Put another way, if the policy isn't deterministic, then the transition probability to state $s'$ at a time $t+1$ when action $a$ is taken for state $s$ at time $t$, is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "P_a(s',s) = P(s_{t+1} = s'|s_t=s,a)\n",
    "\\end{equation}\n",
    "\n",
    "When the policy is deterministic, this transition probability reduces to the following, independent of $a$:\n",
    "\n",
    "\\begin{equation}\n",
    "P(s',s) = P(s_{t+1} = s'|s_t=s)\n",
    "\\end{equation}\n",
    "\n",
    "To be clear, a deterministic policy means that one and only one action will always be selected for a given state $s$, but the **next state $s'$ will still be selected stochastically**.\n",
    "\n",
    "In the general case of RL, it isn't possible to fully know all this information, some of which might be hidden and evolving, so it isn't possible to specify a fully-deterministic policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "Often this **cumulative reward** is computed using the **discounted sum** over all rewards observed:\n",
    "\n",
    "\\begin{equation}\n",
    "\\arg\\max_{\\pi} \\sum_{t=1}^T \\gamma^t R_t(\\pi),\n",
    "\\end{equation}\n",
    "\n",
    "where $T$ is the number of steps taken in the MDP (this is a random variable and may depend on $\\pi$), $R_t$ is the reward received at time $t$ (also a random variable which depends on $\\pi$), and $\\gamma$ is the **discount factor**. The value of $\\gamma$ is between 0 and 1, meaning it has the effect of \"discounting\" earlier rewards vs. more recent rewards. \n",
    "\n",
    "The [Wikipedia page on MDP](https://en.wikipedia.org/wiki/Markov_decision_process) provides more details. Note what we said in the third bullet, that the new state only depends on the previous state and the action taken. The assumption is that we can simplify our effort by ignoring all the previous states except the last one and still achieve good results. This is known as the [Markov property](https://en.wikipedia.org/wiki/Markov_property). This assumption often works well and it greatly reduces the resources required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## The Elements of RL\n",
    "\n",
    "Here are the elements of RL that expand on MDP concepts (see [Sutton 2018](https://mitpress.mit.edu/books/reinforcement-learning-second-edition) for more details):\n",
    "\n",
    "#### Policies\n",
    "\n",
    "Unlike MDP, the **transition function** probabilities are often not known in advance, but must be learned. Learning is done through repeated \"play\", where the agent interacts with the environment.\n",
    "\n",
    "This makes the **policy** $\\pi$ harder to determine. Because the fully state space usually can't be fully known, the choice of action $a$ for given state $s$ almostly always remains a stochastic choice, never deterministic, unlike MDP.\n",
    "\n",
    "#### Reward Signal\n",
    "\n",
    "The idea of a **reward signal** encapsulates the desired goal for the system and provides feedback for updating the policy based on how well particular events or actions contribute rewards towards the goal.\n",
    "\n",
    "#### Value Function\n",
    "\n",
    "The **value function** encapsulates the maximum cumulative reward likely to be achieved starting from a given state for an **episode**. This is harder to determine than the simple reward returned after taking an action. In fact, much of the research in RL over the decades has focused on finding better and more efficient implementations of value functions. To illustrate the challenge, repeatedly taking one sequence of actions may yield low rewards for a while, but eventually provide large rewards. Conversely, always choosing a different sequence of actions may yield a good reward at each step, but be suboptimal for the cumulative reward.\n",
    "\n",
    "#### Episode\n",
    "\n",
    "A sequence of steps by the agent starting in an initial state. At each step, the agent observes the current state, chooses the next action, and receives the new reward. Episodes are used for both training policies and replaying with an existing policy (called _rollout_).\n",
    "\n",
    "#### Model\n",
    "\n",
    "An optional feature, some RL algorithms develop or use a **model** of the environment to anticipate the resulting states and rewards for future actions. Hence, they are useful for _planning_ scenarios. Methods for solving RL problems that use models are called _model-based methods_, while methods that learn by trial and error are called _model-free methods_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "## Reinforcement Learning Example\n",
    "\n",
    "Let's finish this introduction let's learn about the popular \"hello world\" (1) example environment for RL, balancing a pole vertically on a moving cart, called `CartPole`. Then we'll see how to use RLlib to train a policy using a popular RL algorithm, _Proximal Policy Optimization_, again using `CartPole`.\n",
    "\n",
    "(1) In books and tutorials on programming languages, it is a tradition that the very first program shown prints the message \"Hello World!\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "### CartPole and OpenAI\n",
    "\n",
    "The popular [OpenAI \"gym\" environment](https://gym.openai.com/) provides MDP interfaces to a variety of simulated environments. Perhaps the most popular for learning RL is `CartPole`, a simple environment that simulates the physics of balancing a pole on a moving cart. The `CartPole` problem is described at https://gym.openai.com/envs/CartPole-v1. Here is an image from that website, where the pole is currently falling to the right, which means the cart will need to move to the right to restore balance:\n",
    "\n",
    "![Cart Pole](../images/rllib/Cart-Pole.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hideCode": false,
    "hidePrompt": false,
    "id": "bAptEafKNlhm"
   },
   "source": [
    "This example fits into the MDP framework as follows:\n",
    "- The **state** consists of the position and velocity of the cart (moving in one dimension from left to right) as well as the angle and angular velocity of the pole that is balancing on the cart.\n",
    "- The **actions** are to decrease or increase the cart's velocity by one unit. A negative velocity means it is moving to the left.\n",
    "- The **transition function** is deterministic and is determined by simulating physical laws. Specifically, for a given **state**, what should we choose as the next velocity value? In the RL context, the correct velocity value to choose has to be learned. Hence, we learn a _policy_ that approximates the optimal transition function that could be calculated from the laws of physics.\n",
    "- The **reward function** is a constant 1 as long as the pole is upright, and 0 once the pole has fallen over. Therefore, maximizing the reward means balancing the pole for as long as possible.\n",
    "- The **discount factor** in this case can be taken to be 1, meaning we treat the rewards at all time steps equally and don't discount any of them.\n",
    "\n",
    "More information about the `gym` Python module is available at https://gym.openai.com/. The list of all the available Gym environments is in [this wiki page](https://github.com/openai/gym/wiki/Table-of-environments). We'll use a few more of them and even create our own in subsequent lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9Kwo5ZfNlhn"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WPpofaxQNlhp"
   },
   "source": [
    "The code below illustrates how to create and manipulate MDPs in Python. An MDP can be created by calling `gym.make`. Gym environments are identified by names like `CartPole-v1`. A **catalog of built-in environments** can be found at https://gym.openai.com/envs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "6DZ68SG9Nlhp",
    "outputId": "293be60b-8107-42f2-c54a-58f3eaf295f2"
   },
<<<<<<< HEAD
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created env: <TimeLimit<CartPoleEnv<CartPole-v0>>>\n"
     ]
    }
   ],
   "source": [
    "#actually makes the MDP/ framework\n",
    "env = gym.make('CartPole-v0')\n",
=======
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
>>>>>>> 55a6899603adedc2daf2f3dd0a7922f4b4a2b406
    "print('Created env:', env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xn5PqgDzNlhr"
   },
   "source": [
    "Reset the state of the MDP by calling `env.reset()`. This call returns the initial state of the MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "zRA58dOFNlhs",
    "outputId": "7aba4eac-fb0f-4654-eb49-0fbd6d664f28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The starting state is: [-0.03106178 -0.030467    0.00774762 -0.03442081]\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "print('The starting state is:', state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the state is the **position of the cart | its velocity | the angle of the pole | the angular velocity of the pole**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MuXXesWNlhu"
   },
   "source": [
<<<<<<< HEAD
    "The `env.step` method takes **an action**. In the case of the CartPole environment, the appropriate actions are 0 or 1, for pushing the cart to the left or right, respectively. `env.step()` returns a tuple of four things:\n",
=======
    "The `env.step` method takes an action. In the case of the `CartPole` environment, the appropriate actions are 0 or 1, for pushing the cart to the left or right, respectively. `env.step()` returns a tuple of four things:\n",
>>>>>>> 55a6899603adedc2daf2f3dd0a7922f4b4a2b406
    "1. the new state of the environment\n",
    "2. a reward\n",
    "3. a boolean indicating whether the simulation has finished\n",
    "4. a dictionary of miscellaneous extra information\n",
    "\n",
    "Let's show what happens if we take one step with an action of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "hideCode": false,
    "hidePrompt": false,
    "id": "TufVaMz_Nlhu",
    "outputId": "920b9758-7d85-49e8-f8ef-4586b6947dea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03167112 -0.22569919  0.00705921  0.26069646] 1.0 False {}\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "state, reward, done, info = env.step(action)\n",
    "print(state, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBIoIuWYNlhw"
   },
   "source": [
    "A **rollout** is a simulation of a policy in an environment. It is used both during training and when running simulations with a trained policy. \n",
    "\n",
    "The code below performs a rollout in a given environment. It takes **random actions** until the simulation has finished and returns the cumulative reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zp00mr88Nlhw",
    "outputId": "f0d01977-00c9-4ad2-931a-7f9730b5b005"
   },
   "outputs": [],
   "source": [
    "def random_rollout(env):\n",
    "    #resets the state of the environment\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # Keep looping as long as the simulation has not finished. as long as not done == TRUE. \n",
    "    while not done:\n",
    "        # Choose a random action (either 0 or 1). (left or right)\n",
    "        action = np.random.choice([0, 1])\n",
    "        \n",
    "        # Take the action in the environment. envokes the action and then gets outputs on the left\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update the cumulative reward. with non discounting\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try rerunning the following cell a few times. How much do the answers change? Note that the maximum possible reward for `CartPole` is 500. You'll probably get numbers under 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zp00mr88Nlhw",
    "outputId": "f0d01977-00c9-4ad2-931a-7f9730b5b005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.0\n",
      "48.0\n"
     ]
    }
   ],
   "source": [
    "reward = random_rollout(env)\n",
    "print(reward)\n",
    "reward = random_rollout(env)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i3FVvEJRNlhy"
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "Choosing actions at random in `random_rollout` is not a very effective policy, as the previous results showed. Finish implementing the `rollout_policy` function below, which takes an environment *and* a policy. Recall that the *policy* is a function that takes in a *state* and returns an *action*. The main difference is that instead of choosing a **random action**, like we just did (with poor results), the action should be chosen **with the policy** (as a function of the state).\n",
    "\n",
    "**POLICY(STATE) -> ACTION** \n",
    "\n",
    "> **Note:** Exercise solutions for this tutorial can be found [here](solutions/Ray-RLlib-Solutions.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "9PgmkROqNlhy",
    "outputId": "91445278-aeb7-4e86-e1b7-c92e5ba830a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sample policy got an average reward of 9.42.\n",
      "The second sample policy got an average reward of 30.41.\n"
     ]
    }
   ],
   "source": [
    "def rollout_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    # EXERCISE: Fill out this function by copying the appropriate part of 'random_rollout'\n",
    "    # and modifying it to choose the action using the policy.\n",
    "    \n",
    "    while not done:\n",
    "        # Choose a random action (either 0 or 1). (left or right)\n",
    "        action = policy(state) #take in the policy\n",
    "        \n",
    "        # Take the action in the environment. envokes the action and then gets outputs on the left\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Update the cumulative reward. with non discounting\n",
    "        cumulative_reward += reward\n",
    "\n",
    "    # Return the cumulative reward.\n",
    "    return cumulative_reward\n",
    "\n",
    "def sample_policy1(state):\n",
    "    return 0 if state[0] < 0 else 1\n",
    "\n",
    "def sample_policy2(state):\n",
    "    return 1 if state[0] < 0 else 0\n",
    "\n",
    "reward1 = np.mean([rollout_policy(env, sample_policy1) for _ in range(100)])\n",
    "reward2 = np.mean([rollout_policy(env, sample_policy2) for _ in range(100)])\n",
    "\n",
    "print('The first sample policy got an average reward of {}.'.format(reward1))\n",
    "print('The second sample policy got an average reward of {}.'.format(reward2))\n",
    "\n",
    "assert 5 < reward1 < 15, ('Make sure that rollout_policy computes the action '\n",
    "                          'by applying the policy to the state.')\n",
    "assert 25 < reward2 < 35, ('Make sure that rollout_policy computes the action '\n",
    "                           'by applying the policy to the state.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll return to `CartPole` in lesson [01: Application Cart Pole](explore-rllib/01-Application-Cart-Pole.ipynb) in the `explore-rllib` section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXQ8hIB9Nlh0"
   },
   "source": [
    "### RLlib Reinforcement Learning Example: Cart Pole with Proximal Policy Optimization\n",
    "\n",
    "This section demonstrates how to use the _proximal policy optimization_ (PPO) algorithm implemented by [RLlib](http://rllib.io). PPO is a popular way to develop a policy. RLlib also uses [Ray Tune](http://tune.io), the Ray Hyperparameter Tuning framework, which is covered in the [Ray Tune Tutorial](../ray-tune/00-Ray-Tune-Overview.ipynb).\n",
    "\n",
    "We'll provide relatively little explanation of **RLlib** concepts for now, but explore them in greater depth in subsequent lessons. For more on RLlib, see the documentation at http://rllib.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qXQ8hIB9Nlh0"
   },
   "source": [
    "PPO is described in detail in [this paper](https://arxiv.org/abs/1707.06347). It is a variant of _Trust Region Policy Optimization_ (TRPO) described in [this earlier paper](https://arxiv.org/abs/1502.05477). [This OpenAI post](https://openai.com/blog/openai-baselines-ppo/) provides a more accessible introduction to PPO.\n",
    "\n",
    "PPO works in two phases. In the first phase, a large number of rollouts are performed in parallel. The rollouts are then aggregated on the driver and a surrogate optimization objective is defined based on those rollouts. In the second phase, we use SGD (_stochastic gradient descent_) to find the policy that maximizes that objective with a penalty term for diverging too much from the current policy.\n",
    "\n",
    "![PPO](../images/rllib/ppo.png)\n",
    "\n",
    "> **NOTE:** The SGD optimization step is best performed in a data-parallel manner over multiple GPUs. This is exposed through the `num_gpus` field of the `config` dictionary. Hence, for normal usage, one or more GPUs is recommended.\n",
    "\n",
    "(The original version of this example can be found [here](https://raw.githubusercontent.com/ucbrise/risecamp/risecamp2018/ray/tutorial/rllib_exercises/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwPnR2ibNlh2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import gym  # imported above already, but listed here for completeness\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script checks if the Ray cluster is already running. If not, it tells you what to do to start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ray is already running.\n"
     ]
    }
   ],
   "source": [
    "#Go into the working directory /academy\n",
    "# do this in terminal \n",
    "# ./tools/start-ray.sh\n",
    "!../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQFzEX2BNlh3"
   },
   "source": [
    "Now start Ray in this \"driver\" process. This must be done before we instantiate any RL agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQFzEX2BNlh3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.105',\n",
       " 'raylet_ip_address': '192.168.1.105',\n",
       " 'redis_address': '192.168.1.105:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ray intialization\n",
    "#address='auto' gets the addres\n",
    "#ignore_reinit_error=True allows us to reinit with complaining \n",
    "#log_to_driver=False \n",
    "\n",
    "ray.init(address='auto', ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Tip:** Having trouble starting Ray? See the [Troubleshooting](../reference/Troubleshooting-Tips-Tricks.ipynb) tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell prints the URL for the Ray Dashboard. **This is only correct if you are running this tutorial on a laptop.** Click the link to open the dashboard.\n",
    "\n",
    "If you are running on the Anyscale platform, use the URL provided by your instructor to open the Dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard URL: http://localhost:8265\n"
     ]
    }
   ],
   "source": [
    "#RAY DASHBOARD\n",
    "print(f'Dashboard URL: http://{ray.get_webui_url()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9yhpJZVNlh5"
   },
   "source": [
    "Instantiate a PPOTrainer object. We pass in a config object that specifies how the network and training procedure should be configured. Some of the parameters are the following.\n",
    "\n",
    "- `num_workers` is the number of actors that the agent will create. This determines the degree of parallelism that will be used. In a cluster, these actors will be spread over the available nodes. (**Avoid more workers than CPU cores**)\n",
    "- `num_sgd_iter` is the number of epochs of SGD (stochastic gradient descent, i.e., passes through the data) that will be used to optimize the PPO surrogate objective at each iteration of PPO, for each _minibatch_ (\"chunk\") of training data. Using minibatches is more efficient than training with one record at a time.\n",
    "- `sgd_minibatch_size` is the SGD minibatch size (batches of data) that will be used to optimize the PPO surrogate objective.\n",
    "- `model` contains a dictionary of parameters describing the neural net used to parameterize the policy. The `fcnet_hiddens` parameter is a list of the sizes of the hidden layers. Here, we have two hidden layers of size 100, each.\n",
    "- `num_cpus_per_worker` when set to 0 prevents Ray from pinning a CPU core to each worker, which means we could run out of workers in a constrained environment like a laptop or a cloud VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok210MCfNlh5"
   },
   "outputs": [],
   "source": [
    "config = DEFAULT_CONFIG.copy() #defined in the above\n",
    "config['num_workers'] = 4 \n",
    "config['num_sgd_iter'] = 30 #how many iterations \n",
    "config['sgd_minibatch_size'] = 128 #size of chunks\n",
    "config['model']['fcnet_hiddens'] = [100, 100] #a neural network model\n",
    "config['num_cpus_per_worker'] = 1 \n",
    "#Sometime it is slightly more efficient to run less workers with more cores each: \n",
    "#For example 3 workers with 2 cores each or 2 workers with 3 cores each.\n",
    "#If you want to complete a very large assignment quickly allocate all 6 cores to 1 worker. \n",
    "#However, the overall throughput will be up to 25% less than 6 workers with 1 core each.\n",
    "#NOTE: a very large assignment is something like an LL test on an exponent over 100 Million.\n",
    "#https://www.mersenneforum.org/showthread.php?t=22741 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_fake_gpus': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': True,\n",
      " 'clip_param': 0.3,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': 180,\n",
      " 'compress_observations': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'eager': -1,\n",
      " 'eager_tracing': False,\n",
      " 'entropy_coeff': 0.0,\n",
      " 'entropy_coeff_schedule': None,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': 10,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'tf',\n",
      " 'gamma': 0.99,\n",
      " 'grad_clip': None,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'kl_coeff': 0.2,\n",
      " 'kl_target': 0.01,\n",
      " 'lambda': 1.0,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'lr': 5e-05,\n",
      " 'lr_schedule': None,\n",
      " 'memory': 0,\n",
      " 'memory_per_worker': 0,\n",
      " 'metrics_smoothing_episodes': 100,\n",
      " 'min_iter_time_s': 0,\n",
      " 'model': {'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_options': -1,\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [100, 100],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'state_shape': None,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': True,\n",
      "           'zero_mean': True},\n",
      " 'monitor': False,\n",
      " 'multiagent': {'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_mapping_fn': None},\n",
      " 'no_done_at_end': False,\n",
      " 'no_eager_on_workers': False,\n",
      " 'normalize_actions': False,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_sgd_iter': 30,\n",
      " 'num_workers': 4,\n",
      " 'object_store_memory': 0,\n",
      " 'object_store_memory_per_worker': 0,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_max_file_size': 67108864,\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'rollout_fragment_length': 200,\n",
      " 'sample_async': False,\n",
      " 'sample_batch_size': -1,\n",
      " 'seed': None,\n",
      " 'sgd_minibatch_size': 128,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'shuffle_sequences': True,\n",
      " 'simple_optimizer': False,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 0,\n",
      " 'train_batch_size': 4000,\n",
      " 'use_critic': True,\n",
      " 'use_gae': True,\n",
      " 'use_pytorch': -1,\n",
      " 'vf_clip_param': 10.0,\n",
      " 'vf_loss_coeff': 1.0,\n",
      " 'vf_share_layers': False}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ok210MCfNlh5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-18 20:28:11,634\tINFO trainer.py:585 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2020-07-18 20:28:11,635\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2020-07-18 20:28:14,299\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent = PPOTrainer(config, 'CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ty1a6AWVNlh7"
   },
   "source": [
    "Now let's train the policy on the `CartPole-v1` environment for `N` steps. The JSON object returned by each call to `agent.train()` contains a lot of information we'll inspect below. For now, we'll extract information we'll graph, such as `episode_reward_mean`. The _mean_ values are more useful for determining successful training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: Min/Mean/Max reward:   9.0000/ 22.3977/ 65.0000\n",
      "  1: Min/Mean/Max reward:  10.0000/ 36.6697/118.0000\n",
      "  2: Min/Mean/Max reward:  10.0000/ 54.0600/200.0000\n",
      "  3: Min/Mean/Max reward:  10.0000/ 82.7200/200.0000\n",
      "  4: Min/Mean/Max reward:  12.0000/111.9700/200.0000\n",
      "  5: Min/Mean/Max reward:  12.0000/134.5700/200.0000\n",
      "  6: Min/Mean/Max reward:  24.0000/161.3200/200.0000\n",
      "  7: Min/Mean/Max reward:  24.0000/177.3500/200.0000\n",
      "  8: Min/Mean/Max reward:  24.0000/185.6200/200.0000\n",
      "  9: Min/Mean/Max reward:  24.0000/190.5800/200.0000\n"
     ]
    }
   ],
   "source": [
    "N=10\n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "for n in range(N):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    episode = {'n': n, \n",
    "               'episode_reward_min':  result['episode_reward_min'],  \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max':  result['episode_reward_max'],  \n",
    "               'episode_len_mean':    result['episode_len_mean']}    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    print(f'{n:3d}: Min/Mean/Max reward: {result[\"episode_reward_min\"]:8.4f}/{result[\"episode_reward_mean\"]:8.4f}/{result[\"episode_reward_max\"]:8.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert the episode data to a Pandas `DataFrame` for easy manipulation. The results indicate how much reward the policy is receiving (`episode_reward_*`) and how many time steps of the environment the policy ran (`episode_len_mean`). The maximum possible reward for this problem is `500`. The reward mean and trajectory length are very close because the agent receives a reward of one for every time step that it survives. However, this is specific to this environment and not true in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>episode_reward_min</th>\n",
       "      <th>episode_reward_mean</th>\n",
       "      <th>episode_reward_max</th>\n",
       "      <th>episode_len_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>22.397727</td>\n",
       "      <td>65.0</td>\n",
       "      <td>22.397727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>36.669725</td>\n",
       "      <td>118.0</td>\n",
       "      <td>36.669725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54.060000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>54.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>82.720000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>82.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>111.970000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>111.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>134.570000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>134.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>24.0</td>\n",
       "      <td>161.320000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>161.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>24.0</td>\n",
       "      <td>177.350000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>177.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>24.0</td>\n",
       "      <td>185.620000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>185.620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>24.0</td>\n",
       "      <td>190.580000</td>\n",
       "      <td>200.0</td>\n",
       "      <td>190.580000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n  episode_reward_min  episode_reward_mean  episode_reward_max  \\\n",
       "0  0                 9.0            22.397727                65.0   \n",
       "1  1                10.0            36.669725               118.0   \n",
       "2  2                10.0            54.060000               200.0   \n",
       "3  3                10.0            82.720000               200.0   \n",
       "4  4                12.0           111.970000               200.0   \n",
       "5  5                12.0           134.570000               200.0   \n",
       "6  6                24.0           161.320000               200.0   \n",
       "7  7                24.0           177.350000               200.0   \n",
       "8  8                24.0           185.620000               200.0   \n",
       "9  9                24.0           190.580000               200.0   \n",
       "\n",
       "   episode_len_mean  \n",
       "0         22.397727  \n",
       "1         36.669725  \n",
       "2         54.060000  \n",
       "3         82.720000  \n",
       "4        111.970000  \n",
       "5        134.570000  \n",
       "6        161.320000  \n",
       "7        177.350000  \n",
       "8        185.620000  \n",
       "9        190.580000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=episode_data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n',\n",
       " 'episode_reward_min',\n",
       " 'episode_reward_mean',\n",
       " 'episode_reward_max',\n",
       " 'episode_len_mean']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from util.line_plots import plot_line, plot_line_with_min_max, plot_line_with_stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh.io\n",
    "# The next two lines prevent Bokeh from opening the graph in a new window.\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the length and reward means are equal, we'll only plot one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"e377af76-a700-4f21-b1ab-ee29ee398ea2\" data-root-id=\"1004\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"765f1c70-3f92-4867-b2ff-34a0edb906b6\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1012\"}],\"center\":[{\"id\":\"1015\"},{\"id\":\"1019\"},{\"id\":\"1042\"}],\"left\":[{\"id\":\"1016\"}],\"plot_height\":500,\"plot_width\":800,\"renderers\":[{\"id\":\"1035\"},{\"id\":\"1040\"}],\"title\":{\"id\":\"1046\"},\"toolbar\":{\"id\":\"1026\"},\"x_range\":{\"id\":\"1005\"},\"x_scale\":{\"id\":\"1008\"},\"y_range\":{\"id\":\"1003\"},\"y_scale\":{\"id\":\"1010\"}},\"id\":\"1004\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"SaveTool\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1041\",\"type\":\"CDSView\"},{\"attributes\":{\"axis_label\":\"n\",\"formatter\":{\"id\":\"1049\"},\"ticker\":{\"id\":\"1013\"}},\"id\":\"1012\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1025\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1020\"},{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1044\"}]},\"id\":\"1026\",\"type\":\"Toolbar\"},{\"attributes\":{\"axis_label\":\"reward\",\"formatter\":{\"id\":\"1051\"},\"ticker\":{\"id\":\"1017\"}},\"id\":\"1016\",\"type\":\"LinearAxis\"},{\"attributes\":{\"base\":{\"field\":\"n\",\"units\":\"data\"},\"fill_alpha\":0.3,\"level\":\"underlay\",\"line_color\":\"blue\",\"lower\":{\"field\":\"episode_reward_min\",\"units\":\"data\"},\"source\":{\"id\":\"1002\"},\"upper\":{\"field\":\"episode_reward_max\",\"units\":\"data\"}},\"id\":\"1042\",\"type\":\"Band\"},{\"attributes\":{\"overlay\":{\"id\":\"1025\"}},\"id\":\"1022\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"end\":203.82,\"start\":5.18},\"id\":\"1003\",\"type\":\"Range1d\"},{\"attributes\":{\"axis\":{\"id\":\"1016\"},\"dimension\":1,\"grid_line_alpha\":0.5,\"ticker\":null},\"id\":\"1019\",\"type\":\"Grid\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1038\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1039\"},\"selection_glyph\":null,\"view\":{\"id\":\"1041\"}},\"id\":\"1040\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1010\",\"type\":\"LinearScale\"},{\"attributes\":{\"line_alpha\":0.1,\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1039\",\"type\":\"Line\"},{\"attributes\":{\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1038\",\"type\":\"Line\"},{\"attributes\":{},\"id\":\"1054\",\"type\":\"Selection\"},{\"attributes\":{\"data\":{\"episode_len_mean\":{\"__ndarray__\":\"RhdddNFlNkBcnZGKuVVCQEjhehSuB0tArkfhehSuVECuR+F6FP5bQArXo3A90mBACtejcD0qZEAzMzMzMytmQKRwPQrXM2dAw/UoXI/SZ0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[10]},\"episode_reward_max\":{\"__ndarray__\":\"AAAAAABAUEAAAAAAAIBdQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[10]},\"episode_reward_mean\":{\"__ndarray__\":\"RhdddNFlNkBcnZGKuVVCQEjhehSuB0tArkfhehSuVECuR+F6FP5bQArXo3A90mBACtejcD0qZEAzMzMzMytmQKRwPQrXM2dAw/UoXI/SZ0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[10]},\"episode_reward_min\":{\"__ndarray__\":\"AAAAAAAAIkAAAAAAAAAkQAAAAAAAACRAAAAAAAAAJEAAAAAAAAAoQAAAAAAAAChAAAAAAAAAOEAAAAAAAAA4QAAAAAAAADhAAAAAAAAAOEA=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[10]},\"index\":[0,1,2,3,4,5,6,7,8,9],\"level_0\":[0,1,2,3,4,5,6,7,8,9],\"n\":[0,1,2,3,4,5,6,7,8,9]},\"selected\":{\"id\":\"1054\"},\"selection_policy\":{\"id\":\"1053\"}},\"id\":\"1002\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"data_source\":{\"id\":\"1002\"},\"glyph\":{\"id\":\"1033\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1034\"},\"selection_glyph\":null,\"view\":{\"id\":\"1036\"}},\"id\":\"1035\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1053\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"source\":{\"id\":\"1002\"}},\"id\":\"1036\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.3},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"blue\"},\"size\":{\"units\":\"screen\",\"value\":3},\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1033\",\"type\":\"Scatter\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"n\",\"$x\"],[\"episode_reward_mean\",\"$y\"]]},\"id\":\"1044\",\"type\":\"HoverTool\"},{\"attributes\":{\"text\":\"Episode Rewards\"},\"id\":\"1046\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1020\",\"type\":\"PanTool\"},{\"attributes\":{\"axis\":{\"id\":\"1012\"},\"grid_line_alpha\":0.5,\"ticker\":null},\"id\":\"1015\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1017\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"ResetTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"blue\"},\"size\":{\"units\":\"screen\",\"value\":3},\"x\":{\"field\":\"n\"},\"y\":{\"field\":\"episode_reward_mean\"}},\"id\":\"1034\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1021\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1008\",\"type\":\"LinearScale\"}],\"root_ids\":[\"1004\"]},\"title\":\"Bokeh Application\",\"version\":\"2.1.1\"}};\n",
       "  var render_items = [{\"docid\":\"765f1c70-3f92-4867-b2ff-34a0edb906b6\",\"root_ids\":[\"1004\"],\"roots\":{\"1004\":\"e377af76-a700-4f21-b1ab-ee29ee398ea2\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_line_with_min_max(df, x_col='n', y_col='episode_reward_mean', min_col='episode_reward_min', max_col='episode_reward_max',\n",
    "                      title='Episode Rewards', x_axis_label='n', y_axis_label='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../images/rllib/Cart-Pole-Episode-Rewards.png))\n",
    "\n",
    "The model is quickly able to hit the maximum value of 500, but the mean is what's most valueable. After 10 steps, we're more than half way there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI, here are two views of the whole value for one result. First, a \"pretty print\" output.\n",
    "\n",
    "> **Tip:** The output will be long. When this happens for a cell, right click and select _Enable scrolling for outputs_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3o0wjdZ3Nlh7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custom_metrics: {}\n",
      "date: 2020-07-18_20-28-57\n",
      "done: false\n",
      "episode_len_mean: 190.58\n",
      "episode_reward_max: 200.0\n",
      "episode_reward_mean: 190.58\n",
      "episode_reward_min: 24.0\n",
      "episodes_this_iter: 22\n",
      "episodes_total: 502\n",
      "experiment_id: 7599681e0426413691c37f6f2e19d9b1\n",
      "hostname: paul\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      cur_kl_coeff: 0.07500000298023224\n",
      "      cur_lr: 4.999999873689376e-05\n",
      "      entropy: 0.5669357180595398\n",
      "      entropy_coeff: 0.0\n",
      "      kl: 0.0005619224393740296\n",
      "      model: {}\n",
      "      policy_loss: -0.0022857903968542814\n",
      "      total_loss: 387.1932678222656\n",
      "      vf_explained_var: 0.4828588664531708\n",
      "      vf_loss: 387.1954345703125\n",
      "  num_steps_sampled: 40000\n",
      "  num_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 192.168.1.105\n",
      "num_healthy_workers: 4\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.2\n",
      "  ram_util_percent: 26.5\n",
      "pid: 4214\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_env_wait_ms: 0.04545069151864189\n",
      "  mean_inference_ms: 0.5281108431882119\n",
      "  mean_processing_ms: 0.10317195081932297\n",
      "time_since_restore: 16.76654577255249\n",
      "time_this_iter_s: 1.615889549255371\n",
      "time_total_s: 16.76654577255249\n",
      "timers:\n",
      "  learn_throughput: 4283.086\n",
      "  learn_time_ms: 933.906\n",
      "  load_throughput: 1089076.014\n",
      "  load_time_ms: 3.673\n",
      "  sample_throughput: 5513.242\n",
      "  sample_time_ms: 725.526\n",
      "  update_time_ms: 1.658\n",
      "timestamp: 1595068137\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pretty_print(results[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll learn about more of these values as continue the tutorial.\n",
    "\n",
    "The whole, long JSON blob, which includes the historical stats about episode rewards and lengths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': 200.0,\n",
       " 'episode_reward_min': 24.0,\n",
       " 'episode_reward_mean': 190.58,\n",
       " 'episode_len_mean': 190.58,\n",
       " 'episodes_this_iter': 22,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [200.0,\n",
       "   200.0,\n",
       "   192.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   185.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   167.0,\n",
       "   200.0,\n",
       "   136.0,\n",
       "   200.0,\n",
       "   199.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   126.0,\n",
       "   188.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   182.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   61.0,\n",
       "   200.0,\n",
       "   178.0,\n",
       "   24.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   121.0,\n",
       "   57.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   193.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   179.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   198.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   171.0,\n",
       "   200.0,\n",
       "   195.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   136.0,\n",
       "   198.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   172.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0,\n",
       "   200.0],\n",
       "  'episode_lengths': [200,\n",
       "   200,\n",
       "   192,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   185,\n",
       "   200,\n",
       "   200,\n",
       "   167,\n",
       "   200,\n",
       "   136,\n",
       "   200,\n",
       "   199,\n",
       "   200,\n",
       "   200,\n",
       "   126,\n",
       "   188,\n",
       "   200,\n",
       "   200,\n",
       "   182,\n",
       "   200,\n",
       "   200,\n",
       "   61,\n",
       "   200,\n",
       "   178,\n",
       "   24,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   121,\n",
       "   57,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   193,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   179,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   198,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   171,\n",
       "   200,\n",
       "   195,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   136,\n",
       "   198,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   172,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200,\n",
       "   200]},\n",
       " 'sampler_perf': {'mean_env_wait_ms': 0.04545069151864189,\n",
       "  'mean_processing_ms': 0.10317195081932297,\n",
       "  'mean_inference_ms': 0.5281108431882119},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 4,\n",
       " 'timesteps_total': 40000,\n",
       " 'timers': {'sample_time_ms': 725.526,\n",
       "  'sample_throughput': 5513.242,\n",
       "  'load_time_ms': 3.673,\n",
       "  'load_throughput': 1089076.014,\n",
       "  'learn_time_ms': 933.906,\n",
       "  'learn_throughput': 4283.086,\n",
       "  'update_time_ms': 1.658},\n",
       " 'info': {'learner': {'default_policy': {'cur_kl_coeff': 0.07500000298023224,\n",
       "    'cur_lr': 4.999999873689376e-05,\n",
       "    'total_loss': 387.19327,\n",
       "    'policy_loss': -0.0022857904,\n",
       "    'vf_loss': 387.19543,\n",
       "    'vf_explained_var': 0.48285887,\n",
       "    'kl': 0.00056192244,\n",
       "    'entropy': 0.5669357,\n",
       "    'entropy_coeff': 0.0,\n",
       "    'model': {}}},\n",
       "  'num_steps_sampled': 40000,\n",
       "  'num_steps_trained': 40000},\n",
       " 'done': False,\n",
       " 'episodes_total': 502,\n",
       " 'training_iteration': 10,\n",
       " 'experiment_id': '7599681e0426413691c37f6f2e19d9b1',\n",
       " 'date': '2020-07-18_20-28-57',\n",
       " 'timestamp': 1595068137,\n",
       " 'time_this_iter_s': 1.615889549255371,\n",
       " 'time_total_s': 16.76654577255249,\n",
       " 'pid': 4214,\n",
       " 'hostname': 'paul',\n",
       " 'node_ip': '192.168.1.105',\n",
       " 'config': {'num_workers': 4,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'rollout_fragment_length': 200,\n",
       "  'sample_batch_size': -1,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'num_gpus': 0,\n",
       "  'train_batch_size': 4000,\n",
       "  'model': {'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'fcnet_hiddens': [100, 100],\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action_reward': False,\n",
       "   'state_shape': None,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'custom_options': -1},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env_config': {},\n",
       "  'env': 'CartPole-v0',\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 5e-05,\n",
       "  'monitor': False,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'tf',\n",
       "  'eager_tracing': False,\n",
       "  'no_eager_on_workers': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'StochasticSampling'},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 0,\n",
       "  'timesteps_per_iteration': 0,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'memory': 0,\n",
       "  'object_store_memory': 0,\n",
       "  'memory_per_worker': 0,\n",
       "  'object_store_memory_per_worker': 0,\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None},\n",
       "  'use_pytorch': -1,\n",
       "  'eager': -1,\n",
       "  'use_critic': True,\n",
       "  'use_gae': True,\n",
       "  'lambda': 1.0,\n",
       "  'kl_coeff': 0.2,\n",
       "  'sgd_minibatch_size': 128,\n",
       "  'shuffle_sequences': True,\n",
       "  'num_sgd_iter': 30,\n",
       "  'lr_schedule': None,\n",
       "  'vf_share_layers': False,\n",
       "  'vf_loss_coeff': 1.0,\n",
       "  'entropy_coeff': 0.0,\n",
       "  'entropy_coeff_schedule': None,\n",
       "  'clip_param': 0.3,\n",
       "  'vf_clip_param': 10.0,\n",
       "  'grad_clip': None,\n",
       "  'kl_target': 0.01,\n",
       "  'simple_optimizer': False,\n",
       "  '_fake_gpus': False},\n",
       " 'time_since_restore': 16.76654577255249,\n",
       " 'timesteps_since_restore': 0,\n",
       " 'iterations_since_restore': 10,\n",
       " 'perf': {'cpu_util_percent': 75.2, 'ram_util_percent': 26.5}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the runs of the final refined agent. \n",
    "results[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the `episode_reward` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"87757403-0ce9-46bb-90f8-7ecc2b0d0e8a\" data-root-id=\"1937\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"985529cf-2163-46a2-820f-5a29616ce8be\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1945\"}],\"center\":[{\"id\":\"1948\"},{\"id\":\"1952\"}],\"left\":[{\"id\":\"1949\"}],\"plot_height\":500,\"plot_width\":800,\"renderers\":[{\"id\":\"1968\"},{\"id\":\"1973\"}],\"title\":{\"id\":\"1977\"},\"toolbar\":{\"id\":\"1959\"},\"x_range\":{\"id\":\"1938\"},\"x_scale\":{\"id\":\"1941\"},\"y_range\":{\"id\":\"1936\"},\"y_scale\":{\"id\":\"1943\"}},\"id\":\"1937\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1941\",\"type\":\"LinearScale\"},{\"attributes\":{\"line_alpha\":0.1,\"x\":{\"field\":\"episode\"},\"y\":{\"field\":\"reward\"}},\"id\":\"1972\",\"type\":\"Line\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"1958\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"source\":{\"id\":\"1935\"}},\"id\":\"1969\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"tooltips\":[[\"episode\",\"$x\"],[\"reward\",\"$y\"]]},\"id\":\"1975\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"2045\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"data_source\":{\"id\":\"1935\"},\"glyph\":{\"id\":\"1966\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1967\"},\"selection_glyph\":null,\"view\":{\"id\":\"1969\"}},\"id\":\"1968\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"end\":203.4,\"start\":26.6},\"id\":\"1936\",\"type\":\"Range1d\"},{\"attributes\":{},\"id\":\"2043\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"text\":\"Episode Rewards\"},\"id\":\"1977\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"2047\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"data_source\":{\"id\":\"1935\"},\"glyph\":{\"id\":\"1971\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1972\"},\"selection_glyph\":null,\"view\":{\"id\":\"1974\"}},\"id\":\"1973\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1953\",\"type\":\"PanTool\"},{\"attributes\":{\"axis\":{\"id\":\"1949\"},\"dimension\":1,\"grid_line_alpha\":0.5,\"ticker\":null},\"id\":\"1952\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1938\",\"type\":\"DataRange1d\"},{\"attributes\":{\"overlay\":{\"id\":\"1958\"}},\"id\":\"1955\",\"type\":\"BoxZoomTool\"},{\"attributes\":{\"axis\":{\"id\":\"1945\"},\"grid_line_alpha\":0.5,\"ticker\":null},\"id\":\"1948\",\"type\":\"Grid\"},{\"attributes\":{\"source\":{\"id\":\"1935\"}},\"id\":\"1974\",\"type\":\"CDSView\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"blue\"},\"size\":{\"units\":\"screen\",\"value\":3},\"x\":{\"field\":\"episode\"},\"y\":{\"field\":\"reward\"}},\"id\":\"1967\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1946\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1943\",\"type\":\"LinearScale\"},{\"attributes\":{\"x\":{\"field\":\"episode\"},\"y\":{\"field\":\"reward\"}},\"id\":\"1971\",\"type\":\"Line\"},{\"attributes\":{\"axis_label\":\"reward\",\"formatter\":{\"id\":\"2045\"},\"ticker\":{\"id\":\"1950\"}},\"id\":\"1949\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1957\",\"type\":\"SaveTool\"},{\"attributes\":{\"axis_label\":\"episode\",\"formatter\":{\"id\":\"2043\"},\"ticker\":{\"id\":\"1946\"}},\"id\":\"1945\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1956\",\"type\":\"ResetTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1953\"},{\"id\":\"1954\"},{\"id\":\"1955\"},{\"id\":\"1956\"},{\"id\":\"1957\"},{\"id\":\"1975\"}]},\"id\":\"1959\",\"type\":\"Toolbar\"},{\"attributes\":{\"data\":{\"episode\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"index\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"level_0\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"reward\":{\"__ndarray__\":\"AAAAAACAVEAAAAAAAIBdQAAAAAAAQGVAAAAAAAAgYUAAAAAAAGBkQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAgEZAAAAAAACAZEAAAAAAAGBlQAAAAAAAAGlAAAAAAACAYEAAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAACBmQAAAAAAAwFxAAAAAAABAZ0AAAAAAAABpQAAAAAAAAGlAAAAAAAAgZkAAAAAAAABpQAAAAAAA4GFAAAAAAAAAXkAAAAAAAMBfQAAAAAAAwGNAAAAAAAAgYEAAAAAAAIBfQAAAAAAAwFNAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAABAYUAAAAAAAABMQAAAAAAAQF1AAAAAAACgZkAAAAAAAIBlQAAAAAAAwFdAAAAAAAAAZEAAAAAAAEBTQAAAAAAAAGlAAAAAAAAAaUAAAAAAAABpQAAAAAAAwGhAAAAAAAAAaUAAAAAAAMBfQAAAAAAAwGRAAAAAAAAAaUAAAAAAAGBiQAAAAAAA4GNAAAAAAAAAaUAAAAAAAABkQAAAAAAAQGJAAAAAAAAAaUAAAAAAAGBjQAAAAAAAAGlAAAAAAABAXEAAAAAAAABpQAAAAAAAoGFAAAAAAAAAaUAAAAAAAKBkQAAAAAAAAGRAAAAAAAAAaUAAAAAAAEBmQAAAAAAAgGdAAAAAAAAATUAAAAAAAIBXQAAAAAAAQGVAAAAAAAAAaUAAAAAAAABkQAAAAAAAAGlAAAAAAAAAaUAAAAAAAEBgQAAAAAAAAGlAAAAAAACAWkAAAAAAAABpQAAAAAAAAD5AAAAAAACAY0AAAAAAAOBlQAAAAAAAIGFAAAAAAACAZEAAAAAAAOBlQAAAAAAA4GNAAAAAAAAAaUAAAAAAAABpQAAAAAAAAGlAAAAAAADAZ0AAAAAAAABlQAAAAAAAYGBAAAAAAAAgYkAAAAAAAABpQAAAAAAAgF5AAAAAAABgY0AAAAAAAABpQAAAAAAAAGlAAAAAAAAAaUAAAAAAACBiQAAAAAAAAGlAAAAAAAAgY0A=\",\"dtype\":\"float64\",\"order\":\"little\",\"shape\":[100]}},\"selected\":{\"id\":\"2048\"},\"selection_policy\":{\"id\":\"2047\"}},\"id\":\"1935\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1950\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1954\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"2048\",\"type\":\"Selection\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.3},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"blue\"},\"size\":{\"units\":\"screen\",\"value\":3},\"x\":{\"field\":\"episode\"},\"y\":{\"field\":\"reward\"}},\"id\":\"1966\",\"type\":\"Scatter\"}],\"root_ids\":[\"1937\"]},\"title\":\"Bokeh Application\",\"version\":\"2.1.1\"}};\n",
       "  var render_items = [{\"docid\":\"985529cf-2163-46a2-820f-5a29616ce8be\",\"root_ids\":[\"1937\"],\"roots\":{\"1937\":\"87757403-0ce9-46bb-90f8-7ecc2b0d0e8a\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1937"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "episode_rewards = results[-1]['hist_stats']['episode_reward']\n",
    "df_episode_rewards = pd.DataFrame(data={'episode':range(len(episode_rewards)), 'reward':episode_rewards})\n",
    "plot_line(df_episode_rewards, x_col='episode', y_col='reward', title='Episode Rewards', x_axis_label='episode', y_axis_label='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../images/rllib/Cart-Pole-Episode-Rewards2.png))\n",
    "\n",
    "For a well-trained model, most runs do very well while occasional runs do poorly. Try plotting other results episodes by changing the array index in `results[-1]` to another number between `0` and `9`. (The length of `results` is `10`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FPdkWLrENlh9"
   },
   "source": [
    "### Exercise 2\n",
    "\n",
    "The current network and training configuration are too large and heavy-duty for a simple problem like `CartPole`. Modify the configuration to use a smaller network (the `config['model']['fcnet_hiddens']` setting) and to speed up the optimization of the surrogate objective. (Fewer SGD iterations and a larger batch size should help.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lp6tqkNNlh9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-18 21:01:09,806\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# Make edits here:\n",
    "config = DEFAULT_CONFIG.copy() #the shallow copy modifies the file! \n",
    "config['num_workers'] = 4\n",
    "config['num_sgd_iter'] = 10 \n",
    "config['sgd_minibatch_size'] = 256 #chunk size\n",
    "config['model']['fcnet_hiddens'] = [20, 20]\n",
    "config['num_cpus_per_worker'] = 0 #set it to one then it pins a worker to core and hold up that core. \n",
    "\n",
    "agent = PPOTrainer(config, 'CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64FmVP7kNlh_"
   },
   "source": [
    "Train the agent and try to get a reward of 200. If it's training too slowly you may need to modify the config above to use fewer hidden units, a larger `sgd_minibatch_size`, a smaller `num_sgd_iter`, or a larger `num_workers`.\n",
    "\n",
    "This should take around `N` = 20 or 30 training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XB7sdKUzNliA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max reward: 93.0\n",
      "Max reward: 76.0\n",
      "Max reward: 107.0\n",
      "Max reward: 103.0\n",
      "Max reward: 104.0\n",
      "Max reward: 129.0\n",
      "Max reward: 189.0\n",
      "Max reward: 143.0\n",
      "Max reward: 162.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n",
      "Max reward: 190.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n",
      "Max reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "N=20 #training iterations. \n",
    "results = []\n",
    "episode_data = []\n",
    "episode_json = []\n",
    "for n in range(N):\n",
    "    result = agent.train()\n",
    "    results.append(result)\n",
    "    episode = {'n': n, \n",
    "               'episode_reward_mean': result['episode_reward_mean'], \n",
    "               'episode_reward_max': result['episode_reward_max'],  \n",
    "               'episode_len_mean': result['episode_len_mean']}    \n",
    "    episode_data.append(episode)\n",
    "    episode_json.append(json.dumps(episode))\n",
    "    print(f'Max reward: {episode[\"episode_reward_max\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PW6bN9CYNliB"
   },
   "source": [
    "# Using Checkpoints\n",
    "\n",
    "You checkpoint the current state of a trainer to save what it has learned. Checkpoints are used for subsequent _rollouts_ and also to continue training later from a known-good state.  Calling `agent.save()` creates the checkpoint and returns the path to the checkpoint file, which can be used later to restore the current state to a new trainer. Here we'll load the trained policy into the same process, but often it would be loaded in a new process, for example on a production cluster for serving that is separate from the training cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6uf808LMNliC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paul/ray_results/PPO_CartPole-v0_2020-07-18_21-01-07yinaklf2/checkpoint_20/checkpoint-20\n"
     ]
    }
   ],
   "source": [
    "#save what the trainer has learnt\n",
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "05icI8bfNliD"
   },
   "source": [
    "Now load the checkpoint in a new trainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Qq2_AYVNliE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-18 22:21:39,541\tWARNING worker.py:1047 -- WARNING: 12 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n",
      "2020-07-18 22:21:41,460\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n",
      "2020-07-18 22:21:41,552\tINFO trainable.py:423 -- Restored on 192.168.1.105 from checkpoint: /home/paul/ray_results/PPO_CartPole-v0_2020-07-18_21-01-07yinaklf2/checkpoint_20/checkpoint-20\n",
      "2020-07-18 22:21:41,553\tINFO trainable.py:430 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': None, '_time_total': 19.55017614364624, '_episodes_total': 1353}\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "test_agent = PPOTrainer(trained_config, 'CartPole-v1')\n",
    "test_agent.restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c2gUUlqkNliG"
   },
   "source": [
    "Use the previously-trained policy to act in an environment. The key line is the call to `test_agent.compute_action(state)` which uses the trained policy to choose an action. This is an example of _rollout_, which we'll study in a subsequent lesson.\n",
    "\n",
    "Verify that the cumulative reward received roughly matches up with the reward printed above. It will be at or near 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9asL5Z5lNliH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-18 22:59:51,865\tWARNING worker.py:1047 -- WARNING: 12 PYTHON workers have been started. This could be a result of using a large number of actors, or it could be a consequence of using nested tasks (see https://github.com/ray-project/ray/issues/3644) for some a discussion of workarounds.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action = test_agent.compute_action(state)  # key line; get the next action\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n",
    "print(cumulative_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lesson, [02: Introduction to RLlib](02-Introduction-to-RLlib.ipynb) steps back to introduce to RLlib, its goals and the capabilities it provides."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of RLlib Tutorial",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python [conda env:anyscale-academy] *",
   "language": "python",
   "name": "conda-env-anyscale-academy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
