{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib Multi-Armed Bandits - A Simple Bandit Example\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a very simple contextual bandit example with three arms. We'll run trials using RLlib and [Tune](http://tune.io), Ray's hyperparameter tuning library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random, time\n",
    "import ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the bandit as a subclass of an OpenAI Gym environment. We set the action space to have three discrete variables, one action for each arm, and an observation space (the context) in the range -1.0 to 1.0, inclusive. (See the [configuring environments](https://docs.ray.io/en/latest/rllib-env.html#configuring-environments) documentation for more details about creating custom environments.)\n",
    "\n",
    "There are two contexts defined. Note that we'll randomly pick one of them to use when `reset` is called, but it stays fixed (static) throughout the episode (the set of steps between calls to `reset`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBandit (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {        # 2 contexts: -1 and 1\n",
    "            -1.: [-10, 0, 10],\n",
    "            1.: [10, 0, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.]) #pick -1 or 1 \n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBandit(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the definition of `self.rewards_for_context`. For context `-1.`, choosing the **third** arm (index 2 in the array) maximizes the reward, yielding `10.0` for each pull. Similarly, for context `1.`, choosing the **first** arm (index 0 in the array) maximizes the reward. It is never advantageous to choose the second arm.\n",
    "\n",
    "We'll see if our training results agree ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try repeating the next two code cells enough times to see the `current_context` set to `1.0` and `-1.0`, which is initialized randomly in `reset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initial observation = [-1.  1.], bandit = SimpleContextualBandit(action_space=Discrete(3), observation_space=Box(2,), current_context=1.0, rewards per context={-1.0: [-10, 0, 10], 1.0: [10, 0, -10]})'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit = SimpleContextualBandit()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bandit.current_context` and the observation of the current environment will remain fixed through the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_context = 1.0\n",
      "observation = [-1.  1.], action = 2, reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [-1.  1.], action = 2, reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [-1.  1.], action = 0, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [-1.  1.], action = 2, reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [-1.  1.], action = 2, reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [-1.  1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n",
      "observation = [-1.  1.], action = 0, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [-1.  1.], action = 0, reward =   10, done = True , info = {'regret': 0}\n",
      "observation = [-1.  1.], action = 2, reward =  -10, done = True , info = {'regret': 20}\n",
      "observation = [-1.  1.], action = 1, reward =    0, done = True , info = {'regret': 10}\n"
     ]
    }
   ],
   "source": [
    "print(f'current_context = {bandit.current_context}')\n",
    "for i in range(10):\n",
    "    action = bandit.action_space.sample()\n",
    "    observation, reward, done, info = bandit.step(action)\n",
    "    print(f'observation = {observation}, action = {action}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the `current_context`. If it's `1.0`, does the `0` (first) action yield the highest reward and lowest regret? If it's `-1.0`, does the `2` (third) action yield the highest reward and lowest regret? The `1` (second) action always returns `0` reward, so it's never optimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LinUCB\n",
    "\n",
    "For this simple example, we can easily determine the best actions to take. Let's see how well our system does. We'll train with [LinUCB](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-upper-confidence-bound-contrib-linucb), a linear version of _Upper Confidence Bound_, for the exploration-exploitation strategy. _LinUCB_ assumes a linear dependency between the expected reward of an action and its context. Recall that a linear function is of the form $z = ax + by + c$, for example, where $x$, $y$, and $z$ are variables and $a$, $b$, and $c$ are constants. _LinUCB_ models the representation space using a set of linear predictors. Hence, the $Q_t(a)$ _value_ function discussed for UCB in the [previous lesson](02-Exploration-vs-Exploitation-Strategies.ipynb) is assumed to be a linear function here.\n",
    "\n",
    "Look again at how we defined `rewards_for_context`. Is it linear as expected for _LinUCB_?\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Yes, for each arm, the reward is linear in the context. For example, the first arm has a reward of `-10` for context `-1.0` and `10` for context `1.0`. Crucially, the _same_ linear function that works for the first arm will work for the other two arms if you multiplied the constants in the linear function by `0` and `-1`, respectively. Hence, we expect _LinUCB_ to work well for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use Tune to train the policy for this bandit. But first, we want to make sure that Ray is running and we want to initialize Ray in this \"driver\" program explicitly. \n",
    "\n",
    "The following shell command will probably print _INFO: Ray is already running._ If not, follow the instructions it prints to start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ray is already running.\n"
     ]
    }
   ],
   "source": [
    "!../../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now initialize Ray in this \"driver\" program (notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.105',\n",
       " 'raylet_ip_address': '192.168.1.105',\n",
       " 'redis_address': '192.168.1.105:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.progress_reporter import JupyterNotebookReporter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `ray.tune.run` below would handle Ray initialization for us, if Ray isn't already running. If you want to prevent this and have Tune exit with an error when Ray isn't already initialized, then pass `ray_auto_init=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.3/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/4 CPUs, 0/2 GPUs, 0.0/8.06 GiB heap, 0.0/2.78 GiB objects<br>Result logdir: /home/paul/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                       </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit_65d05_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m 2020-07-19 00:46:02,197\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m 2020-07-19 00:46:02,240\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 00:46:02,753\tERROR trial_runner.py:520 -- Trial contrib_LinUCB_SimpleContextualBandit_65d05_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 430, in fetch_result\n",
      "    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/worker.py\", line 1474, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::LinUCB.train()\u001b[39m (pid=17278, ip=192.168.1.105)\n",
      "  File \"python/ray/_raylet.pyx\", line 446, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 400, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 500, in train\n",
      "    raise e\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 486, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\n",
      "    result = self._train()\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 132, in _train\n",
      "    return self._train_exec_impl()\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 170, in _train_exec_impl\n",
      "    res = next(self.train_exec_impl)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 731, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 814, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 814, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 847, in apply_flatten\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 799, in add_wait_hooks\n",
      "    item = next(it)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/execution/rollout_ops.py\", line 70, in sampler\n",
      "    yield workers.local_worker().sample()\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 528, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 59, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 164, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 500, in _env_runner\n",
      "    tf_sess=tf_sess)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 827, in _do_policy_eval\n",
      "    timestep=policy.global_timestep)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/policy/torch_policy.py\", line 151, in compute_actions\n",
      "    input_dict, state_batches, seq_lens)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/models/modelv2.py\", line 180, in __call__\n",
      "    res = self.forward(restored, state or [], seq_lens)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 157, in forward\n",
      "    x, sample_theta=False, use_ucb=True)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 126, in predict\n",
      "    [self.arms[i](x, sample_theta) for i in range(self.num_outputs)],\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 126, in <listcomp>\n",
      "    [self.arms[i](x, sample_theta) for i in range(self.num_outputs)],\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 80, in forward\n",
      "    self._check_inputs(x)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 86, in _check_inputs\n",
      "    assert x.ndim in [2, 3], \\\n",
      "AttributeError: 'Tensor' object has no attribute 'ndim'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.5/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/2 GPUs, 0.0/8.06 GiB heap, 0.0/2.78 GiB objects<br>Result logdir: /home/paul/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                       </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit_65d05_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                       </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit_65d05_00000</td><td style=\"text-align: right;\">           1</td><td>/home/paul/ray_results/contrib/LinUCB/contrib_LinUCB_SimpleContextualBandit_0_2020-07-19_00-46-00_376v7hp/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [contrib_LinUCB_SimpleContextualBandit_65d05_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5028747f2110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJupyterNotebookReporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# This is the default, actually.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;31m# Change to 0 or 1 to reduce the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         \u001b[0mray_auto_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0;31m# Don't allow Tune to initialize Ray.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                        )\n",
      "\u001b[0;32m~/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, fail_fast, restore, search_alg, scheduler, with_server, server_port, verbose, progress_reporter, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [contrib_LinUCB_SimpleContextualBandit_65d05_00000])"
     ]
    }
   ],
   "source": [
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=2,     # Change to 0 or 1 to reduce the output.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A lot of output is printed with `verbose` set to `2`. Use `0` for no output and `1` for short summaries.)\n",
    "\n",
    "How long did it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The trials took 32.95785140991211 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some of the final data as a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to inspect the progression of training is to use TensorBoard.\n",
    "\n",
    "1. If you are runnng on the Anyscale Platform, click the _TensorBoard_ link. \n",
    "2. If you running this notebook on a laptop, open a terminal window using the `+` under the _Edit_ menu, run the following command, then open the URL shown.\n",
    "\n",
    "```\n",
    "tensorboard --logdir ~/ray_results \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have many data sets plotted from previous tutorial lessons. In the _Runs_ on the left, look for one named something like this:\n",
    "\n",
    "```\n",
    "contrib/LinUCB/contrib_LinUCB_SimpleContextualBandit_0_YYYY-MM-DD_HH-MM-SSxxxxxxxx  \n",
    "```\n",
    "\n",
    "If you have several of them, you want the one with the latest timestamp. To select just that one, click _toggler all runs_ below the list of runs, then select the one you want. You should see something like [this image](../../images/rllib/TensorBoard1.png).\n",
    "\n",
    "The graph for the metric we were optimizing, the mean reward, is shown with a rectangle surrounding it. It improved steadily during the training runs. For this simple example, the reward mean is easily found in 200 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the the `step` method to randomly change the `current_context` on each invocation:\n",
    "\n",
    "```python\n",
    "def step(self, action):\n",
    "    result = super().step(action)\n",
    "    self.current_context = random.choice([-1.,1.])\n",
    "    return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "            {\n",
    "                \"regret\": 10 - reward\n",
    "            })\n",
    "```\n",
    "\n",
    "Repeat the training and analysis. Does the training behavior change in any appreciable way? Why or why not?\n",
    "\n",
    "See the [solutions notebook](solutions/Multi-Armed-Bandits-Solutions.ipynb) for discussion of this and the following exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Recall the `rewards_for_context` we used:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "We said that Linear Upper Confidence Bound assumes a linear dependency between the expected reward of an action and its context. It models the representation space using a set of linear predictors.\n",
    "\n",
    "Change the values for the rewards as follows, so they no longer have the same simple linear relationship:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 10, 0],\n",
    "    1.: [0, 10, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Run the training again and look at the results for the reward mean in TensorBoard. How successful was the training? How smooth is the plot for `episode_reward_mean`? How many steps were taken in the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 (Optional)\n",
    "\n",
    "We briefly discussed another algorithm for selecting the next action, _Thompson Sampling_, in the [previous lesson](02-Exploration-vs-Exploitation-Strategies.ipynb). Repeat exercises 1 and 2 using linear version, called _Linear Thompson Sampling_ ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints)). To make this change, look at this code we used above:\n",
    "\n",
    "```python\n",
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=1)\n",
    "```\n",
    "\n",
    "Change `contrib/LinUCB` to `contrib/LinTS`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue exploring usage of _LinUCB_ in the next lesson, [04 Linear Upper Confidence Bound](04-Linear-Upper-Confidence-Bound.ipynb) and _LinTS_ in the following lesson, [05 Thompson Sampling](05-Linear-Thompson-Sampling.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
