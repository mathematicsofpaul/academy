{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray RLlib - Multi-Armed Bandits - Exercise Solutions\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a very simple contextual bandit example with 3 arms. We'll run trials using RLlib and [Tune](http://tune.io), Ray's hyperparameter tuning library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, random\n",
    "import ray\n",
    "from ray.tune.progress_reporter import JupyterNotebookReporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Ray is already running.\n"
     ]
    }
   ],
   "source": [
    "!../../../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.105',\n",
       " 'raylet_ip_address': '192.168.1.105',\n",
       " 'redis_address': '192.168.1.105:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270/sockets/raylet',\n",
       " 'webui_url': 'localhost:8265',\n",
       " 'session_dir': '/tmp/ray/session_2020-07-18_20-05-29_369775_4270'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Change the the `step` method to randomly change the `current_context` on each invocation:\n",
    "\n",
    "```python\n",
    "def step(self, action):\n",
    "    result = super().step(action)\n",
    "    self.current_context = random.choice([-1.,1.])\n",
    "    return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "            {\n",
    "                \"regret\": 10 - reward\n",
    "            })\n",
    "```\n",
    "\n",
    "Repeat the training and analysis. Does the training behavior change in any appreciable way? Why or why not?\n",
    "\n",
    "See the [solutions notebook](solutions/Multi-Armed-Bandits-Solutions.ipynb) for discussion of this and the following exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBandit2 (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {\n",
    "            -1.: [-10, 0, 10],\n",
    "            1.: [10, 0, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        self.current_context = random.choice([-1.,1.])\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBandit2(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Initial observation = [ 1. -1.], bandit = SimpleContextualBandit2(action_space=Discrete(3), observation_space=Box(2,), current_context=-1.0, rewards per context={-1.0: [-10, 0, 10], 1.0: [10, 0, -10]})'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit = SimpleContextualBandit2()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = {\n",
    "    \"training_iteration\": 200,\n",
    "    \"timesteps_total\": 100000,\n",
    "    \"episode_reward_mean\": 10.0,\n",
    "}\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.2/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1/4 CPUs, 0/2 GPUs, 0.0/8.06 GiB heap, 0.0/2.78 GiB objects<br>Result logdir: /home/paul/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                        </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit2_b6ad7_00000</td><td>RUNNING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m 2020-07-19 00:41:08,384\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=17169)\u001b[0m 2020-07-19 00:41:08,424\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-19 00:41:08,934\tERROR trial_runner.py:520 -- Trial contrib_LinUCB_SimpleContextualBandit2_b6ad7_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/tune/trial_runner.py\", line 468, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/tune/ray_trial_executor.py\", line 430, in fetch_result\n",
      "    result = ray.get(trial_future[0], DEFAULT_GET_TIMEOUT)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/worker.py\", line 1474, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::LinUCB.train()\u001b[39m (pid=17169, ip=192.168.1.105)\n",
      "  File \"python/ray/_raylet.pyx\", line 446, in ray._raylet.execute_task\n",
      "  File \"python/ray/_raylet.pyx\", line 400, in ray._raylet.execute_task.function_executor\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 500, in train\n",
      "    raise e\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/agents/trainer.py\", line 486, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/tune/trainable.py\", line 261, in train\n",
      "    result = self._train()\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 132, in _train\n",
      "    return self._train_exec_impl()\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/agents/trainer_template.py\", line 170, in _train_exec_impl\n",
      "    res = next(self.train_exec_impl)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 731, in __next__\n",
      "    return next(self.built_iterator)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 814, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 814, in apply_filter\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 847, in apply_flatten\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 799, in add_wait_hooks\n",
      "    item = next(it)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/util/iter.py\", line 744, in apply_foreach\n",
      "    for item in it:\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/execution/rollout_ops.py\", line 70, in sampler\n",
      "    yield workers.local_worker().sample()\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 528, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 59, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 164, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 500, in _env_runner\n",
      "    tf_sess=tf_sess)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/evaluation/sampler.py\", line 827, in _do_policy_eval\n",
      "    timestep=policy.global_timestep)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/policy/torch_policy.py\", line 151, in compute_actions\n",
      "    input_dict, state_batches, seq_lens)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/models/modelv2.py\", line 180, in __call__\n",
      "    res = self.forward(restored, state or [], seq_lens)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 157, in forward\n",
      "    x, sample_theta=False, use_ucb=True)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 126, in predict\n",
      "    [self.arms[i](x, sample_theta) for i in range(self.num_outputs)],\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 126, in <listcomp>\n",
      "    [self.arms[i](x, sample_theta) for i in range(self.num_outputs)],\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 489, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 80, in forward\n",
      "    self._check_inputs(x)\n",
      "  File \"/home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/rllib/contrib/bandits/models/linear_regression.py\", line 86, in _check_inputs\n",
      "    assert x.ndim in [2, 3], \\\n",
      "AttributeError: 'Tensor' object has no attribute 'ndim'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 7.4/15.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/4 CPUs, 0/2 GPUs, 0.0/8.06 GiB heap, 0.0/2.78 GiB objects<br>Result logdir: /home/paul/ray_results/contrib/LinUCB<br>Number of trials: 1 (1 ERROR)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                        </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit2_b6ad7_00000</td><td>ERROR   </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br>Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                                        </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                          </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>contrib_LinUCB_SimpleContextualBandit2_b6ad7_00000</td><td style=\"text-align: right;\">           1</td><td>/home/paul/ray_results/contrib/LinUCB/contrib_LinUCB_SimpleContextualBandit2_0_2020-07-19_00-41-07gyn53xz_/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "('Trials did not complete', [contrib_LinUCB_SimpleContextualBandit2_b6ad7_00000])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9d0ece4d6a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mJupyterNotebookReporter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# This is the default, actually.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m              \u001b[0;31m# Change to 0 or 1 to reduce the output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                         ray_auto_init=False)    # Don't allow Tune to initialize Ray.\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The trials took\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"seconds\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, sync_on_checkpoint, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, fail_fast, restore, search_alg, scheduler, with_server, server_port, verbose, progress_reporter, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTuneError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trials did not complete: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincomplete_trials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTuneError\u001b[0m: ('Trials did not complete', [contrib_LinUCB_SimpleContextualBandit2_b6ad7_00000])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m /home/paul/anaconda3/envs/anyscale-academy/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m 2020-07-19 00:46:02,197\tINFO trainer.py:612 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(pid=17278)\u001b[0m 2020-07-19 00:46:02,240\tWARNING util.py:37 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It trains just as easily as the original implementation that didn't switch contexts between steps. Is this surprising? Probably not, because the relationship between the reward and the context remains linear, so what LinUCB learns for one context is correct for the second context, too. Also, _Tune_ runs many episodes, so it studies both contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 2\n",
    "\n",
    "Recall the `rewards_for_context` we used:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 0, 10],\n",
    "    1.: [10, 0, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "We said that Linear Upper Confidence Bound assumes a linear dependency between the expected reward of an action and its context. It models the representation space using a set of linear predictors.\n",
    "\n",
    "Change the values for the rewards as follows, so they no longer have the same simple linear relationship:\n",
    "\n",
    "```python\n",
    "self.rewards_for_context = {\n",
    "    -1.: [-10, 10, 0],\n",
    "    1.: [0, 10, -10],\n",
    "}\n",
    "```\n",
    "\n",
    "Also remove the change made for exercise 1, the line `self.current_context = random.choice([-1.,1.])` in the `step` method.\n",
    "\n",
    "Run the training again and look at the results for the reward mean in TensorBoard. How successful was the training? How smooth is the plot for `episode_reward_mean`? How many steps were taken in the training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleContextualBanditNonlinear (gym.Env):\n",
    "    def __init__ (self, config=None):\n",
    "        self.action_space = Discrete(3)     # 3 arms\n",
    "        self.observation_space = Box(low=-1., high=1., shape=(2, ), dtype=np.float64)  # Random (x,y), where x,y from -1 to 1\n",
    "        self.current_context = None\n",
    "        self.rewards_for_context = {   # Changed here:\n",
    "            -1.: [-10, 10, 0],\n",
    "            1.: [0, 10, -10],\n",
    "        }\n",
    "\n",
    "    def reset (self):\n",
    "        self.current_context = random.choice([-1., 1.])\n",
    "        return np.array([-self.current_context, self.current_context])\n",
    "\n",
    "    def step (self, action):\n",
    "        reward = self.rewards_for_context[self.current_context][action]\n",
    "        return (np.array([-self.current_context, self.current_context]), reward, True,\n",
    "                {\n",
    "                    \"regret\": 10 - reward\n",
    "                })\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'SimpleContextualBanditNonlinear(action_space={self.action_space}, observation_space={self.observation_space}, current_context={self.current_context}, rewards per context={self.rewards_for_context})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBanditNonlinear()\n",
    "observation = bandit.reset()\n",
    "f'Initial observation = {observation}, bandit = {repr(bandit)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'current_context = {bandit.current_context}')\n",
    "for i in range(10):\n",
    "    action = bandit.action_space.sample()\n",
    "    observation, reward, done, info = bandit.step(action)\n",
    "    print(f'observation = {observation}, action = {action}, reward = {reward:4d}, done = {str(done):5s}, info = {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBanditNonlinear,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It ran the maximum of 20,000 steps and the best it does falls between 4.8 to 5.2 (for different runs), not 10.0. the `episode_reward_mean` is chaotic:\n",
    "\n",
    "![Nonlinear model with LinUCB](../../../images/rllib/TensorBoard2.png).\n",
    "\n",
    "Because LinUCB expcts a linear relationship between the context and each reward, it's not surprising that it fails to converge to the desired reward mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03: Simple Multi-Armed Bandits - Exercise 3\n",
    "\n",
    "We briefly discussed another algorithm for selecting the next action, _Thompson Sampling_, in the [previous lesson](../02-Exploration-vs-Exploitation-Strategies.ipynb). Repeat exercises 1 and 2 using linear version, called _Linear Thompson Sampling_ ([RLlib documentation](https://docs.ray.io/en/latest/rllib-algorithms.html?highlight=greedy#linear-thompson-sampling-contrib-lints)). To make this change, look at this code we used above:\n",
    "\n",
    "```python\n",
    "analysis = tune.run(\"contrib/LinUCB\", config=config, stop=stop, \n",
    "                    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                    verbose=2)  # Change to 0 or 1 to reduce the output.\n",
    "```\n",
    "\n",
    "Change `contrib/LinUCB` to `contrib/LinTS`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBandit2()\n",
    "observation = bandit.reset()\n",
    "\n",
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBandit2,\n",
    "}\n",
    "\n",
    "analysis = ray.tune.run(\"contrib/LinTS\", config=config, stop=stop, \n",
    "    progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "    verbose=1)\n",
    "\n",
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the training only takes 200 steps and converge to the desired reward mean of `10.0`.\n",
    "\n",
    "Now let's try the nonlinear bandit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bandit = SimpleContextualBanditNonlinear()\n",
    "observation = bandit.reset()\n",
    "\n",
    "# `stop` defined above is unchanged.\n",
    "\n",
    "config = {\n",
    "    \"env\": SimpleContextualBanditNonlinear,\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "analysis = ray.tune.run(\"contrib/LinTS\", config=config, stop=stop, \n",
    "                        progress_reporter=JupyterNotebookReporter(overwrite=False),  # This is the default, actually.\n",
    "                        verbose=1)\n",
    "\n",
    "print(\"The trials took\", time.time() - start_time, \"seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run with Thompson sampling yields similar results with the reward mean between 4.5 and 5.0, with somewhat chaotic results over the 20000 steps, if you look at the `episode_reward_mean` graph in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04: Linear Upper Confidence Bound - Exercise 1\n",
    "\n",
    "Change the `training_iterations` from 20 to 40. Does the characteristic behavior of cumulative regret change at higher steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.contrib.bandits.agents.lin_ucb import UCB_CONFIG\n",
    "from ray.rllib.contrib.bandits.envs import ParametricItemRecoEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCB_CONFIG[\"env\"] = ParametricItemRecoEnv\n",
    "\n",
    "# Actual training_iterations will be 40 * timesteps_per_iteration (100 by default) = 4,000\n",
    "training_iterations = 40\n",
    "\n",
    "print(\"Running training for %s time steps\" % training_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(\n",
    "    \"contrib/LinUCB\",\n",
    "    config=UCB_CONFIG,\n",
    "    stop={\"training_iteration\": training_iterations},\n",
    "    num_samples=5,\n",
    "    checkpoint_at_end=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.DataFrame()\n",
    "\n",
    "for key, df in analysis.trial_dataframes.items():\n",
    "    frame = frame.append(df, ignore_index=True)\n",
    "\n",
    "df = frame.groupby(\"info/num_steps_trained\")[\n",
    "    \"info/learner/default_policy/cumulative_regret\"].aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../..') # ... and line_plot functions from \"util\"\n",
    "from util.line_plots import plot_line, plot_line_with_stddev, plot_line_with_min_max\n",
    "\n",
    "sys.path.append(\"..\")          # So we can load the bokeh_util from the parent directory...\n",
    "from bokeh_util import plot_cumulative_regret, plot_wheel_bandit_model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next two lines prevent Bokeh from opening the graph in a new window.\n",
    "import bokeh\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_regret(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinUCB-cumulative-regret2.png))\n",
    "\n",
    "The slope appears to stop flattening, suggesting that the previous number of steps, 2000, was sufficient to get the optimal behavior. Beyond that, regret continues to accumulate, but it's linear in the number of steps, neither getting better or worse.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05: Linear Thompson Sampling - Exercise 1\n",
    "\n",
    "Experiment with different $\\delta$ values, for example 0.7 and 0.9. What do the cumulative regret and weights graphs look like? \n",
    "\n",
    "You can set the $\\delta$ value like this:\n",
    "\n",
    "```python\n",
    "TS_CONFIG[\"delta\"] = 0.7\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.contrib.bandits.agents import LinTSTrainer\n",
    "from ray.rllib.contrib.bandits.agents.lin_ts import TS_CONFIG\n",
    "from ray.rllib.contrib.bandits.envs import WheelBanditEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TS_CONFIG[\"env\"] = WheelBanditEnv\n",
    "\n",
    "training_iterations = 20\n",
    "print(\"Running training for %s time steps\" % training_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ts(delta):\n",
    "    TS_CONFIG[\"delta\"] = delta\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    analysis = ray.tune.run(\n",
    "        LinTSTrainer,\n",
    "        config=TS_CONFIG,\n",
    "        stop={\"training_iteration\": training_iterations},\n",
    "        num_samples=2,\n",
    "        checkpoint_at_end=True,\n",
    "        verbose=1)\n",
    "\n",
    "    print(\"The trials took\", time.time() - start_time, \"seconds\\n\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for key, df_trial in analysis.trial_dataframes.items():\n",
    "        df = df.append(df_trial, ignore_index=True)\n",
    "\n",
    "    return df, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, analysis):\n",
    "    ts_regrets = df \\\n",
    "        .groupby(\"info/num_steps_trained\")[\"info/learner/default_policy/cumulative_regret\"] \\\n",
    "        .aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "    \n",
    "    trial = analysis.trials[0]\n",
    "    trainer = LinTSTrainer(config=TS_CONFIG)\n",
    "    trainer.restore(trial.checkpoint.value)\n",
    "    \n",
    "    model = trainer.get_policy().model\n",
    "    means = [model.arms[i].theta.numpy() for i in range(5)]\n",
    "    covs = [model.arms[i].covariance.numpy() for i in range(5)]\n",
    "\n",
    "    return ts_regrets, model, means, covs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = 0.7\n",
    "ts_df7, analysis7 = run_ts(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_regrets7, model7, means7, covs7 = process_df(ts_df7, analysis7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_regret(ts_regrets7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinTS-Cumulative-Regret-07.png))\n",
    "\n",
    "The cumulative regret values are much higher than for $\\delta = 0.5$ in the lesson, and the standard deviation is ... well crazy. We mentioned in the lesson that the problem becomes harder for higher $\\delta$, which fits this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wheel_bandit_model_weights(means7, covs7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinTS-Weight-Distribution-of-Arms-07.png))\n",
    "\n",
    "Compare to the separation of the clusters compared to $\\delta = 0.5$:\n",
    "\n",
    "![image](../../../images/rllib/LinTS-Weight-Distribution-of-Arms-05.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "delta = 0.9\n",
    "ts_df9, analysis9 = run_ts(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ts_regrets9, model9, means9, covs9 = process_df(ts_df9, analysis9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_regret(ts_regrets9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinTS-Cumulative-Regret-09.png))\n",
    "\n",
    "Qualitatively the same as for $\\delta = 0.7$, but the size of the cumulative regret values are even higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wheel_bandit_model_weights(means9, covs9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/LinTS-Weight-Distribution-of-Arms-09.png))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06 Market Example - Exercise 1\n",
    "\n",
    "Try using a `LinUCBTrainer`-based trainer. How does the annualized return compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some properties we'll need:\n",
    "DEFAULT_MAX_INFLATION = 100.0\n",
    "DEFAULT_TICKERS = [\"sp500\", \"t.bill\", \"t.bond\", \"corp\"]\n",
    "DEFAULT_DATA_FILE = os.path.abspath(os.path.curdir) + '/../market.tsv'  # full path\n",
    "\n",
    "def load_market_data (file_name):\n",
    "    with open(file_name, \"r\") as f:\n",
    "        return pd.read_table(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_market_data(DEFAULT_DATA_FILE)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_years = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.trainer import with_base_config, with_common_config\n",
    "from ray.rllib.contrib.bandits.agents.lin_ucb import UCB_CONFIG\n",
    "from ray.rllib.contrib.bandits.agents.lin_ucb import LinUCBTrainer\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketBandit (gym.Env):\n",
    "\n",
    "    def __init__ (self, config={}):\n",
    "        self.max_inflation = config.get('max-inflation', DEFAULT_MAX_INFLATION)\n",
    "        self.tickers = config.get('tickers', DEFAULT_TICKERS)\n",
    "        self.data_file = config.get('data-file', DEFAULT_DATA_FILE)\n",
    "        print(f\"MarketBandit: max_inflation: {self.max_inflation}, tickers: {self.tickers}, data file: {self.data_file} (config: {config})\")\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Box(\n",
    "            low  = -self.max_inflation,\n",
    "            high =  self.max_inflation,\n",
    "            shape=(1, )\n",
    "        )\n",
    "        self.df = load_market_data(self.data_file)\n",
    "        self.cur_context = None\n",
    "\n",
    "\n",
    "    def reset (self):\n",
    "        self.year = self.df[\"year\"].min()\n",
    "        self.cur_context = self.df.loc[self.df[\"year\"] == self.year][\"inflation\"][0]\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "\n",
    "        return [self.cur_context]\n",
    "\n",
    "\n",
    "    def step (self, action):\n",
    "        if self.done:\n",
    "            reward = 0.\n",
    "            regret = 0.\n",
    "        else:\n",
    "            row = self.df.loc[self.df[\"year\"] == self.year]\n",
    "\n",
    "            # calculate reward\n",
    "            ticker = self.tickers[action]\n",
    "            reward = float(row[ticker])\n",
    "\n",
    "            # calculate regret\n",
    "            max_reward = max(map(lambda t: float(row[t]), self.tickers))\n",
    "            regret = round(max_reward - reward)\n",
    "\n",
    "            # update the context\n",
    "            self.cur_context = float(row[\"inflation\"])\n",
    "\n",
    "            # increment the year\n",
    "            self.year += 1\n",
    "\n",
    "            if self.year >= self.df[\"year\"].max():\n",
    "                self.done = True\n",
    "\n",
    "        context = [self.cur_context]\n",
    "        #context = self.observation_space.sample()\n",
    "\n",
    "        self.info = {\n",
    "            \"regret\": regret,\n",
    "            \"year\": self.year\n",
    "        }\n",
    "\n",
    "        return [context, reward, self.done, self.info]\n",
    "\n",
    "\n",
    "    def seed (self, seed=None):\n",
    "        \"\"\"Sets the seed for this env's random number generator(s).\n",
    "        Note:\n",
    "            Some environments use multiple pseudorandom number generators.\n",
    "            We want to capture all such seeds used in order to ensure that\n",
    "            there aren't accidental correlations between multiple generators.\n",
    "        Returns:\n",
    "            list<bigint>: Returns the list of seeds used in this env's random\n",
    "              number generators. The first value in the list should be the\n",
    "              \"main\" seed, or the value which a reproducer should pass to\n",
    "              'seed'. Often, the main seed equals the provided 'seed', but\n",
    "              this won't be true if seed=None, for example.\n",
    "        \"\"\"\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_config = with_base_config(UCB_CONFIG, {\n",
    "    \"env\":           MarketBandit,\n",
    "    'max-inflation': DEFAULT_MAX_INFLATION,\n",
    "    'tickers':       DEFAULT_TICKERS,\n",
    "    'data-file':     DEFAULT_DATA_FILE\n",
    "})\n",
    "\n",
    "stop = {\n",
    "    \"training_iteration\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MarketLinUCBTrainer = LinUCBTrainer.with_updates(\n",
    "    name=\"MarketLinUCBTrainer\",\n",
    "    default_config=market_config,      # Will be merged with Trainer.COMMON_CONFIG (rllib/agent/trainer.py)\n",
    "    #default_policy=[somePolicyClass]  # If we had a policy...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = ray.tune.run(\n",
    "    MarketLinUCBTrainer,\n",
    "    config=market_config,\n",
    "    stop=stop,\n",
    "    num_samples=3,    \n",
    "    checkpoint_at_end=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts = pd.DataFrame()\n",
    "\n",
    "for key, df_trial in analysis.trial_dataframes.items():\n",
    "    df_ts = df_ts.append(df_trial, ignore_index=True)\n",
    "    \n",
    "df_ts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = df_ts \\\n",
    "    .groupby(\"info/num_steps_trained\")[\"episode_reward_mean\"] \\\n",
    "    .aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrets = df_ts \\\n",
    "    .groupby(\"info/num_steps_trained\")[\"info/learner/default_policy/cumulative_regret\"] \\\n",
    "    .aggregate([\"mean\", \"max\", \"min\", \"std\"])\n",
    "\n",
    "regrets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for _LinTS_ were ~570 for reward mean and the regret stayed under 10000. So, training with _LinUCB_ isn't as successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line_with_stddev(rewards, x_col='info/num_steps_trained', y_col='mean', stddev_col='std', \n",
    "                      title='Rewards vs. Steps', x_axis_label='step', y_axis_label='reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/Market-Bandit-Rewards-vs-Steps-LinUCB.png))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cumulative_regret(regrets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "([image](../../../images/rllib/Market-Bandit-Cumulative-Regret-LinUCB.png))\n",
    "\n",
    "What's the annualized return?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:5.2f}% optimized return annualized\".format(max(rewards[\"mean\"]) / n_years))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is likely the same as the completely random choices investigated in the lesson or worse!!\n",
    "\n",
    "The market that we're modeling doesn't exhibit a linear relationship between the context, inflation in our case, and the rewards. Hence, it's not too surprising that a linear algorithm would fail to model the behavior perfectly. What's interesting here is that Thompson Sampling did a noticeably better job than Upper Confidence Bound."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
