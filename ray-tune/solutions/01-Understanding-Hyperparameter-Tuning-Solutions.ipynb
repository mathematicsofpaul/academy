{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune Tutorial - 01: Understanding Hyperparameter Tuning - Exercise Solution\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)\n",
    "\n",
    "Unlike for previous tutorials, we use separate notebooks for the Tune tutorial solutions, because most of the exercises take a long time to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Try More Neural Network Sizes\n",
    "\n",
    "Repeat the experiment above using the sizes `[20, 40, 60, 80, 100]` or some subset of these numbers, depending on how long you are willing to wait. What combination appears to be best, given the considerations we discussed above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up everything we need from the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [20, 40, 60, 80, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will take around 20-30 minutes, even on a fast laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"episode_reward_mean\": 400},\n",
    "\n",
    "    config={\n",
    "        \"env\": \"CartPole-v1\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 3,\n",
    "        \"model\": {\n",
    "            'fcnet_hiddens': [\n",
    "                tune.grid_search(sizes),\n",
    "                tune.grid_search(sizes)\n",
    "            ]\n",
    "        },\n",
    "        \"eager\": False,\n",
    "    },\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "First, how long did this take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one performed best based on our stopping criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_best_config(metric=\"episode_reward_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that the best result is for the smallest size for the first layer and the largest size for the second layer, but recall what we said in the lesson about all values providing good results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort by `timesteps_total` to see which ones were fastest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values('timesteps_total', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['episode_reward_mean', 'training_iteration', 'timesteps_total', 'config/model']].sort_values('timesteps_total', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the largest networks trained the fastest, `[100,100]`, `[60,100]`, and `[80,100]`, followed closely by some smaller configurations. The larger networks would be easier to train, because they more parameters, but the larger parameter sets would increase training times, but apparently not enough to tip the balance against them. \n",
    "\n",
    "However, the differences are still relatively small compared to our previous pick of `[40,40]`. If you compare the timestamp values, the training time for `[40,40]` is about 20% slower."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
