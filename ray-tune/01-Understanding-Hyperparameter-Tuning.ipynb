{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Tune - Understanding Hyperparameter Tuning\n",
    "\n",
    "Â© 2019-2020, Anyscale. All Rights Reserved\n",
    "\n",
    "![Anyscale Academy](../images/AnyscaleAcademy_Logo_clearbanner_141x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the [Ray RLlib](../ray-rllib/00-Ray-RLlib-Overview.ipynb) lessons used [Ray Tune](http://tune.io) to train policies. This meant we trained _parameters_ that defined the policies. Now we'll learn that Ray Tune was actually designed to determine the best _hyperparameters_ for the problem, before training to determine the _parameters_.\n",
    "\n",
    "This lesson introduces the concepts of _Hyperparameter Tuning or Optimization_ (HPO) and works through a nontrivial example using Tune. \n",
    "\n",
    "See also the [Hyperparameter Tuning References](References-Hyperparameter-Tuning.ipynb) notebook and the [Tune documentation](http://tune.io), in particular, the [API reference](https://docs.ray.io/en/latest/tune/api_docs/overview.html). \n",
    "\n",
    "A recent [Ray Summit Connect](https://anyscale.com/blog/videos-and-slides-for-the-second-ray-summit-connect-june-17-2020/) talk by the creator of Tune, Richard Liaw, provides an excellent overview of the challenges of hyperparameter tuning and how Tune addresses these challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Hyperparameters?\n",
    "\n",
    "In _supervised learning_, we train a model with labeled data so the model can properly label new data values. Everything about the model is defined by a set of _parameters_, such as the weights in a linear regression. \n",
    "\n",
    "In contrast, the _hyperparameters_<sup>1</sup> define structural details about the kind of model itself, like whether or not we are using a linear regression or what architecture is best for a neural network, etc. Other quantities considered hyperparameters include learning rates, discount rates, etc. If we want our training process and resulting model to work well, we first need to determine the optimal or near-optimal set of hyperparameters.\n",
    "\n",
    "How do we determine the optimal hyperparameters? The most straightfoward approach is to perform a loop where we pick a candidate set of values from some reasonably inclusive list of possible values, train a model, compare the results achieved with previous loop iterations, and pick the set that performed best. This process is called _Hyperparameter Tuning_ or _Optimization_ (HPO).\n",
    "\n",
    "This simple algorithm can quickly become very expensive, however. Training a single neural networks can be compute intensive and the space of all possible architectures is huge. Hence, much of the research in hyperparameter tuning, especially for neural networks, focuses on ways to optimize HPO, such as early stopping and pruning the search space when some combinations appear to perform poorly.\n",
    "\n",
    "1. _Hyperparameter_ is often spelled _hyper parameter_, but we'll use the spelling with no space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Simple Example: $k$-Means \n",
    "\n",
    "Let's start with a very simple example of HPO, finding $k$ in $k$-means. \n",
    "\n",
    "The $k$-means algorithm finds clusters in a data set. It's a canonical example of _unsupervised learning_, where information is extracted from a data set, rather than using labeled data to train a model for labelling new data, as in _supervised learning_. We won't discuss the algorithm details, but the essense of it involves a \"guess\" for the expected number of clusters, the $k$ value, then calculating $k$ centroids (the coordinates at the center), one per cluster, along with determining to which cluster each data point belongs. The details are in [$k$-means Wikipedia article](https://en.wikipedia.org/wiki/K-means_clustering). The following animation shows the algorithm in action for a two-dimensional data set where three clusters are evident.\n",
    "\n",
    "![K-Means Convergence](../images/tune/K-means_convergence.gif)\n",
    "\n",
    "(source: [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering). [Larger Image](https://en.wikipedia.org/wiki/K-means_clustering#/media/File:K-means_convergence.gif))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While it is easy to see the clusters in this two-dimensional data set, that won't be for arbitrary datasets, especially those with more than three dimensions. Hence, we should determine the best $k$ value by trying many values and picking the value that appears to be best. In this case, \"best\" would mean that we minimize the distances between the datapoints and centroids. \n",
    "\n",
    "With just one hyperparameter, this problem is comparatively simple and brute force calculations to find the optimal $k$ is usually good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HPO for Neural Networks\n",
    "\n",
    "Where HPO really becomes a challenge is finding the right neural network architecture for your problem. Why are neural networks a challenge. Consider this image of a typical architecture:\n",
    "\n",
    "![Typical Neural Network](../images/tune/hpo-neural-network-example.png)\n",
    "\n",
    "Every number you see is a hyperparameter! So are the decisions about how many layers to have, what kind of layer to use for each layer, etc. The space of possible hyperparameters is enormous, too big to explore naively.\n",
    "\n",
    "So called _neural architecture search_ (NAS) has become a research field in its own right, along with general research in optimizing HPO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Ray Tune\n",
    "\n",
    "[Ray Tune](http://tune.io) is the Ray-based library for hyperparameter tuning. Tune makes it nearly as easy to run distributed, parallelized HPO as it is to run trials on a single machine manually, one after the other. \n",
    "\n",
    "Tune is built as an extensible, pluggable framework, with built-in integrations for many frameworks, including [OpenAI Gym environments](https://gym.openai.com/envs/), [PyTorch](https://pytorch.org), [TensorFlow](http://tensorflow.org), and recently, [sci-kit learn](https://scikit-learn.org/stable/) (see [this recent blog post](https://medium.com/distributed-computing-with-ray/gridsearchcv-2-0-new-and-improved-ee56644cbabf)).\n",
    "\n",
    "Tune also integrates implementations of many state-of-the-art [search algorithms](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html) and [schedulers](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html), so it is easy to optimize your HPO process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Ray Tune\n",
    "\n",
    "We used Ray Tune in several of the reinforcement learning lessons, where we actually didn't optimize any hyperparameters, we just used it to drive the RLlib training process. Now we'll see an example of HPO with Tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to make sure that Ray is running and we want to initialize Ray in this \"driver\" program explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shell command will probably print _INFO: Ray is already running._ If not, follow the instructions it prints to start Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!../tools/start-ray.sh --check --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start Ray in this \"driver\" program (notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(address='auto', ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our [RLlib Simple Multi-Armed Bandit](../ray-rllib/multi-armed-bandits/03-Simple-Multi-Armed-Bandit.ipynb) lesson, we used Tune to train RLlib, but not train hyperparameters. \n",
    "\n",
    "Now we'll use Tune to train hyperparameters. We'll use the same experimental environment we used in our [RLlib `CartPole` lesson](../ray-rllib/explore-rllib/01-Application-Cart-Pole.ipynb). \n",
    "\n",
    "> **Aside:** In case you haven't gone through the [Ray RLlib tutorial](../ray-rllib/00-Ray-RLlib-Overview.ipynb), [CartPole](https://gym.openai.com/envs/CartPole-v1/) is an [OpenAI Gym](https://gym.openai.com/envs/) environment that simulates a cart that moves left or right while attempting to balance a vertical pole. A _policy_ is trained to optimize changing the velocity up and down to control right and left movement with the goal of keeping the pole balanced for as long as possible.\n",
    "\n",
    "The most important hyperparameters for `CartPole` are for the neural network used to learn how to balance the pole. A simple network suffices. We'll use a fully-connected network with two hidden layers, but use Tune to find an optimal choice for the sizes of the layers. To keep the computation tractable for our purposes, we'll just pick sizes from the list of 20 and 40. \n",
    "\n",
    "It's probably that some specific number between these 10 and 100 is optimal for one layer and a different number is optimal for the other layer. However, the computation required for trying all possible combinations is $O(n^2)$. You should consider trying more numbers if you don't mind waiting!\n",
    "\n",
    "However, the two we've chosen are good enough as we'll see. We'll consider what \"good enough\" means when we look at the results.\n",
    "\n",
    "The next cell runs Tune for this purpose. The comments explain what each argument does. We'll do four tries, one for each combination of the two possible values for the two hidden layers.\n",
    "\n",
    "> **Note:** `tune.run` will handle Ray initialization for us, if it isn't already initialized. To force Tune to throw an error instead, pass the argument `ray_auto_init=False`.\n",
    "\n",
    "The next cell will take 5-6 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis = tune.run(\n",
    "    \"PPO\",                                    # Use proximal policy optimization to train \n",
    "    stop={\"episode_reward_mean\": 400},        # Stopping criteria, when average reward over the episodes\n",
    "                                              # of training equals 400 out of a maximum possible 500 score.\n",
    "    config={\n",
    "        \"env\": \"CartPole-v1\",                 # Tune can associate this string with the environment.\n",
    "        \"num_gpus\": 0,                        # If you have GPUs, go for it!\n",
    "        \"num_workers\": 3,                     # Number of Ray workers to use; Use one LESS than \n",
    "                                              # the number of cores you wan to use (or omit this argument)!\n",
    "        \"model\": {                            # The NN model we'll optimize.\n",
    "            'fcnet_hiddens': [                # \"Fully-connected network with N hidden layers\".\n",
    "                tune.grid_search([20, 40]),   # Try these four values for layer one.\n",
    "                tune.grid_search([20, 40])    # Try these four values for layer one.\n",
    "            ]\n",
    "        },\n",
    "        \"eager\": False,                       # Flag for TensorFlow; don't use eager evaluation.\n",
    "    },\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Results\n",
    "\n",
    "First, how long did it take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which one performed best based on our stopping criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_best_config(metric=\"episode_reward_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the smallest combination is good enough, even the best according to this metric! \n",
    "\n",
    "Let's look at the data for all of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = analysis.dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated above, all four combinations actually work about equally well, as far as the `episode_reward_mean` mean is concerned. So what's actually best? What's \"good enough\" in this case?\n",
    "\n",
    "It's useful to consider the `training_iteration` (roughly proportional to `episodes_total`) and `timesteps_total`. Let's project out the most interesting data so we can see it more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['episode_reward_mean', 'training_iteration', 'timesteps_total', 'config/model']].sort_values('timesteps_total', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from this table that the `[20,20]` hyperparameter set took the *most* training iterations, which is understandable as it is the least powerful network configuration. The corresponding number of timesteps was the longest. In contrast, `[40,20]` and `[40,40]` are the fastest to train with almost the same `episode_reward_mean` value.\n",
    "\n",
    "Since all four combinations perform equally well, perhaps it's best to choose the largest network as it trains the fastest. If we need to train the neural network frequently, then fast training times might be most important. This also suggests that we should be sure the trial sizes we used are really best. In a real-world application, you would want to spend more time on HPO, trying a larger set of possible values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the Results with TensorBoard \n",
    "\n",
    "Here is the directory where the training data was written for the \"best\" hyperparameter set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.get_best_logdir(metric=\"episode_reward_mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a separate directory for each trial run.\n",
    "\n",
    "The easiest way to inspect all the training data, including graphs, is to use [TensorBoard](https://www.tensorflow.org/tensorboard).\n",
    "\n",
    "1. If you are runnng on the Anyscale Platform, click the _TensorBoard_ link. \n",
    "2. If you running this notebook on a laptop, open a terminal window using the `+` under the _Edit_ menu, run the following command, then open the URL shown in the output.\n",
    "\n",
    "```\n",
    "tensorboard --logdir ~/ray_results \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a screen shot of TensorBoard after the previous Tune run, showing how _scalars_ like the `episdoe_reward_mean` evolved. Some data shown is for runs with trial values of 60 and 80:\n",
    "\n",
    "![TensorBoard](../images/tune/TensorBoard-CartPole-HPO-scalars.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table of the hyperparameter data, similar to what we saw above by looking at the `analysis.dataframe()`:\n",
    "\n",
    "![TensorBoard](../images/tune/TensorBoard-CartPole-HPO-hyperparameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Useful Information \n",
    "\n",
    "The `ray.tune.trial.Trial` object ([documentation](https://docs.ray.io/en/latest/tune/api_docs/internals.html?highlight=Trial#trial-docstring)) records lots of information about particular trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = analysis.get_best_trial(metric=\"episode_reward_mean\")\n",
    "trial, type(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial.stopping_criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trial.metric_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How long did it take??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = analysis.stats()\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis.runner_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used grid search here, which is a naÃ¯ve approach. In subsequent lessons, we'll explore how to optimize the search process using some of Tune's built-in algorithms for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Try More Neural Network Sizes\n",
    "\n",
    "Repeat the experiment above using the sizes `[20, 40, 60, 80, 100]` or some subset of these numbers, depending on how long you are willing to wait. (It takes about 25 minutes on a recent-vintage laptop.) What combination appears to be best, given the considerations we discussed above?\n",
    "\n",
    "> **Note:** The exercise solution for this tutorial can be found [here](solutions/01-Understanding-Hyperparameter-Tuning-Solutions.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
